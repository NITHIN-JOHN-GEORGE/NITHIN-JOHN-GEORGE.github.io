[{"content":"üîó Medium In this blog you will learn how to provision EKS Kubernetes cluster at any scale using Karpenter . We will also utilize **spot instances **to reduce costs by up to 90 percent. Additionally, we‚Äôll set up the **AWS ALB Ingress Controller **and Amazon EBS CSI driver, establish trust between an OIDC-compatible identity provider and your AWS account using IAM OIDC identity provider (including the necessary IAM roles and policies), install the metrics server, and set up required EKS add-ons using Terraform. We will deploy all of this using Helm charts and configure custom values.yml using Terraform as well. Finally, we will use Terraform Cloud for remote state, creating workspaces, and the plan/apply workflow.\nIf you would like to skip the theoretical explanations please fell free scroll down to the actual terraform configurations and where do the testings!!\nKuberenetes Autoscaling A Kubernetes Cluster is a group of node machines that run containerized applications. Inside these nodes, Pods run containers that demand resources such as CPU, memory, and sometimes disk or GPU.\nGetting the size of a Kubernetes cluster right is not an easy task, if the number of nodes provisioned is too high, resources might be underutilized and if it‚Äôs too low, new workloads won‚Äôt be able to be scheduled in the cluster.\nSetting the number of nodes manually is a simple approach, but it requires manual intervention every time the cluster needs to grow or to shrink, and it will make nearly impossible to adapt the cluster size to cover rapid traffic and load fluctuations.\nOne of the benefits of using Kubernetes is its ability to dynamically scale your infrastructure based on user demand.\nKubernetes offers multiple layers of auto-scaling functionality, including:\nPod-based autoscaling\nHorizontal Pod Autoscaler ‚Äî adds or removes more pods to the deployment as needed\nVertical Pod Autoscaler ‚Äî resizes pod‚Äôs CPU and memory requests and limits to match the load\nNode-based autoscaling: adding or removing nodes as needed\nCluster Autoscaler\nKarpenter\nHow we can reduce cost with Using Spot Instances? AWS offers a cost-saving option for EC2 instances called ‚ÄúSpot instances‚Äù where AWS provides the unused/unoccupied EC2 instances for up to a 90% cheaper than regular On-Demand instances. However, their availability is not guaranteed as they are unused instances that are made available to users.\nWhen a Spot instance is reallocated, AWS automatically provides a new instance to take its place. While Spot instances are similar to regular On-Demand instances, they come with a risk of potential interruption, but they can be an effective way to save costs for workloads that are not time-sensitive.\nSpot instances are compatible with Amazon Elastic Kubernetes Service (EKS), and they can be used to scale production applications when there is a surge in demand. By leveraging Spot instances, users can take advantage of cost savings while also ensuring that their applications can handle spikes in traffic without compromising performance.\nOne downside of using Spot Instances is that they can be interrupted by the AWS EC2 Spot service, which is why it‚Äôs crucial to design your application with fault-tolerance in mind. To help with this, you can utilize Spot Instance interruption notices, which provide a two-minute warning before Amazon EC2 stops or terminates your instance. Keep in mind that after this notification, the instance will be reclaimed. By designing your application to handle interruptions gracefully, you can minimize the impact of Spot Instance interruptions and still benefit from the cost savings they offer.\nWhy use Karpenter instead of Cluster Autoscaler? Cluster Autoscaler is a useful Kubernetes tool that can adjust the size of a Kubernetes cluster by adding or removing nodes, based on the utilization metrics of nodes and the presence of pending pods. However, it requires nodes with the same capacity to function correctly.\nTo adjust the capacity, Kubernetes Cluster Autoscaler interacts with the Autoscaling group service directly. To properly work , AWS EKS managed node groups are needed, and the Autoscaler only scales up or down the managed node groups through Amazon EC2 Auto Scaling Groups. Whenever a new node group is added, it is essential to inform Cluster Autoscaler about it, as there is a mapping between Cluster Autoscaler, which is Kubernetes native, and the node group, which is AWS native.\nCluster Autoscaler does not provide flexibility to handle hundreds of instance types, zones, and purchase options.\nUnlike Autoscaler, Karpenter doesn‚Äôt rely on node groups ,it manages each instance directly. Instead, autoscaling configurations are specified in provisioners, which can be seen as a more customizable alternative to EKS-managed node groups.\nIf you are uncertain about the instance types you require or have no specific requirements, Karpenter can make the decision for you. All you need to do is create a provisioner that outlines the minimum parameter requirements.\nKarpenter is designed to make efficient use of the full range of instance types available through AWS. Unlike other autoscaling tools that may be limited in the instance types they can use, Karpenter can select and utilize any instance type that meets the needs of incoming pods.\nKarpenter looks at the workload (i.e pods) and launches the right instances for the situation.\nKarpenter manages instances directly, without the need for additional orchestration mechanisms such as node groups. This allows it to retry capacity requests in a matter of milliseconds, significantly reducing the time it takes to address capacity issues.Moreover, Karpenter‚Äôs direct management approach enables it to make efficient use of diverse instance types, availability zones, and purchase options, without the need to create numerous node groups. This results in a more flexible and cost-effective solution for managing Kubernetes clusters on AWS.\nInfrastructure costs are reduced by looking for under-utilized nodes and removing them, also by replacing expensive nodes with cheaper alternatives, and by consolidating workloads onto more efficient compute resources.\nKarpenter operates on an intent-based model when making instance selection decisions. This model takes into account the specific resource requests and scheduling requirements of incoming pods. By doing so, Karpenter can identify the most suitable instance type to handle these pods, which may involve requesting a larger instance type capable of accommodating all the pods on a single node.\nFor instance, if there are 50 pending pods waiting to be scheduled, Karpenter will calculate the resource requirements of these pods and select an appropriate instance type to accommodate them. Unlike Cluster Autoscaler, which may need to scale several nodes to meet the demand, Karpenter can request a single large EC2 instance and place all the pods on it.\nHow it works? Karpenter is designed to observe events within the Kubernetes cluster and send commands to the underlying cloud provider‚Äôs compute service. Upon installation, Karpenter begins to monitor the specifications of unschedulable Pods and calculates the aggregate resource requests. Based on this information, it can make decisions to launch and terminate nodes in order to reduce scheduling latencies and infrastructure costs.\nTo achieve this, Karpenter leverages a Custom Resource Definition (CRD) called Provisioner. The Provisioner specifies the node provisioning configuration, including the instance size/type, topology (such as availability zone), architecture (e.g. arm64, amd64), and lifecycle type (such as spot, on-demand, preemptible).\nKarpenter is designed to automatically manage the lifecycle of nodes provisioned by Kubernetes. When a node is no longer needed or has exceeded its TTL (time-to-live), Karpenter triggers a finalization process that includes several steps to gracefully decommission the node.\nOne event that can trigger finalization is the expiration of a node‚Äôs configuration, as defined by the ttlSecondsUntilExpired parameter. This parameter specifies the maximum amount of time that a node can remain active before being terminated. When the node\u0026rsquo;s TTL expires, Karpenter takes action by cordoning off the node, which means that no new workloads can be scheduled on it. Karpenter then begins to drain the pods running on the node, which involves moving the workloads to other nodes in the cluster. Finally, Karpenter terminates the underlying compute resource associated with the node and deletes the node object.\nAnother event that can trigger finalization is when the last workload running on a Karpenter provisioned node is terminated. In this case, Karpenter follows the same process of cordoning the node, draining the pods, terminating the underlying resource, and deleting the node object.\nBy automating the finalization process, Karpenter ensures that nodes are only active when they are needed, which can help to optimize the performance and cost-effectiveness of Kubernetes clusters on AWS.\nHow Karpenter handles Spot Interruptions Karpenter requires a queue to exist that receives event messages from EC2 and health services in order to handle interruption messages properly for nodes\nKarpenter will keep watching for upcoming involuntary interruption events that would cause disruption to your workloads.\nThese events include Spot Interruption Warnings, Scheduled Change Health Events, Instance Terminating Events, and Instance Stopping Events.\nWhen Karpenter detects that one of these events is imminent, it automatically performs cordoning, draining, and termination of the affected node(s) to allow enough time for cleanup of workloads prior to compute disruption. This is particularly useful in scenarios where the terminationGracePeriod for workloads is long or where cleanup is critical, as it ensures that there is enough time to gracefully clean up pods.\nKarpenter achieves this by monitoring an SQS queue that receives critical events from AWS services that could potentially affect nodes. However, to use this feature, it is necessary to provision an SQS queue and add EventBridge rules and targets that forward interruption events from AWS services to the SQS queue.\nNow lets get into action !!! I am assuming that the underlying VPC and network resources are already created.\nI will be creating a seperate blog on setting up Production Grade VPC (HA) with environment-specific run method which we used .\nThe public subnets in the VPC must be tagged with :\n\u0026quot;kubernetes.io/cluster/\u0026lt;cluster-name\u0026gt;\u0026quot; = \u0026quot;owned\u0026quot; \u0026quot;karpenter.sh/discovery\u0026quot; = \u0026lt;cluster-name\u0026gt; \u0026quot;kubernetes.io/role/elb\u0026quot; = 1 The private subnets in the VPC must be tagged with :\n\u0026quot;kubernetes.io/cluster/\u0026lt;cluster-name\u0026gt;\u0026quot; = \u0026quot;owned\u0026quot; \u0026quot;karpenter.sh/discovery\u0026quot; = \u0026lt;cluster-name\u0026gt; \u0026quot;kubernetes.io/role/internal-elb\u0026quot; = 1 Also we will be adding the following tags to the security group of control plane and eks-nodes:\n\u0026quot;karpenter.sh/discovery\u0026quot; = \u0026lt;cluster-name\u0026gt; We will be discussing this when we move ahead!!!\nCreating the EC2 spot Linked Role This step is only necessary if this is the first time you‚Äôre using EC2 Spot in this account. It is necessary to exist in your account in order to let you launch Spot instances.\naws iam create-service-linked-role --aws-service-name spot.amazonaws.com Configuring the EKS Cluster !! Next we will be deploying the cluster itself first , which has deployed a minimum set of On-Demand instances that we will use to deploy Kubernetes controllers on it.\nAfter that we will use Karpenter to deploy Spot instances to showcase a few of the benefits of running a group-less auto scaler.\nTerraform code for Setting up the entire stack can be found at :\nhttps://github.com/NITHIN-JOHN-GEORGE/eks-karpenter-controllers-spot-terraform\nCreating Terraform files For the Setup !! Provider Configuration\nterraform { required_version = \u0026quot;~\u0026gt; 1.0\u0026quot; required_providers { aws = { source = \u0026quot;hashicorp/aws\u0026quot; version = \u0026quot;~\u0026gt; 4.16.0\u0026quot; } kubernetes = { source = \u0026quot;hashicorp/kubernetes\u0026quot; version = \u0026quot;~\u0026gt; 2.18.0\u0026quot; } kubectl = { source = \u0026quot;gavinbunney/kubectl\u0026quot; version = \u0026quot;\u0026gt;= 1.7.0\u0026quot; } helm = { source = \u0026quot;hashicorp/helm\u0026quot; version = \u0026quot;2.8.0\u0026quot; } } } provider \u0026quot;aws\u0026quot; { region = var.region access_key = var.AWS_ACCESS_KEY secret_key = var.AWS_SECRET_KEY } output \u0026quot;endpoint\u0026quot; { value = data.aws_eks_cluster.cluster.endpoint } # output \u0026quot;kubeconfig-certificate-authority-data\u0026quot; { # value = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data) # } provider \u0026quot;kubernetes\u0026quot; { host = data.aws_eks_cluster.cluster.endpoint cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data) token = data.aws_eks_cluster_auth.cluster.token } provider \u0026quot;kubectl\u0026quot; { host = data.aws_eks_cluster.cluster.endpoint cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data) token = data.aws_eks_cluster_auth.cluster.token load_config_file = false } provider \u0026quot;helm\u0026quot; { kubernetes { host = data.aws_eks_cluster.cluster.endpoint token = data.aws_eks_cluster_auth.cluster.token cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data) } } EKS Cluster Setup\nlocals { env = [\u0026quot;prod\u0026quot;, \u0026quot;qa\u0026quot;, \u0026quot;dev\u0026quot;] } #------------Security group for eks-cluster-------------------------------------------------- resource \u0026quot;aws_security_group\u0026quot; \u0026quot;control_plane_sg\u0026quot; { count = contains(local.env, var.env) ? 1 : 0 name = \u0026quot;k8s-control-plane-sg\u0026quot; vpc_id = \u0026quot;${data.aws_vpc.vpc.id}\u0026quot; tags = { Name = \u0026quot;k8s-control-plane-sg\u0026quot; vpc_id = \u0026quot;${data.aws_vpc.vpc.id}\u0026quot; ManagedBy = \u0026quot;terraform\u0026quot; Env = var.env \u0026quot;karpenter.sh/discovery\u0026quot; = var.cluster_name } } #----------------------Security group traffic rules for eks cluster------------------------------------------ ## Ingress rule resource \u0026quot;aws_security_group_rule\u0026quot; \u0026quot;control_plane_inbound\u0026quot; { description = \u0026quot;Allow worker Kubelets and pods to receive communication from the cluster control plane\u0026quot; security_group_id = element(aws_security_group.control_plane_sg.*.id,0) type = \u0026quot;ingress\u0026quot; from_port = 0 to_port = 65535 protocol = \u0026quot;tcp\u0026quot; cidr_blocks = flatten([\u0026quot;${values(data.aws_subnet.public_subnets).*.cidr_block}\u0026quot;, \u0026quot;${values(data.aws_subnet.private_subnets).*.cidr_block}\u0026quot;]) } ## Egress rule resource \u0026quot;aws_security_group_rule\u0026quot; \u0026quot;control_plane_outbound\u0026quot; { security_group_id = element(aws_security_group.control_plane_sg.*.id,0) type = \u0026quot;egress\u0026quot; from_port = 0 to_port = 65535 protocol = \u0026quot;-1\u0026quot; cidr_blocks = [\u0026quot;0.0.0.0/0\u0026quot;] } #------------Security group for eks-nodes-------------------------------------------------- resource \u0026quot;aws_security_group\u0026quot; \u0026quot;eks-nodes\u0026quot; { count = contains(local.env, var.env) ? 1 : 0 name = \u0026quot;nodes_eks_sg\u0026quot; description = \u0026quot;nodes_eks_sg\u0026quot; vpc_id = \u0026quot;${data.aws_vpc.vpc.id}\u0026quot; egress { from_port = 0 to_port = 0 protocol = \u0026quot;-1\u0026quot; cidr_blocks = [\u0026quot;0.0.0.0/0\u0026quot;] } tags = { Name = \u0026quot;nodes_eks_sg\u0026quot; vpc_id = \u0026quot;${data.aws_vpc.vpc.id}\u0026quot; ManagedBy = \u0026quot;terraform\u0026quot; Env = var.env \u0026quot;karpenter.sh/discovery\u0026quot; = var.cluster_name } } #---------Security group traffic rules for node-security-group---------------- ## Ingress rule resource \u0026quot;aws_security_group_rule\u0026quot; \u0026quot;nodes_ssh\u0026quot; { security_group_id = element(aws_security_group.eks-nodes.*.id,0) type = \u0026quot;ingress\u0026quot; from_port = 22 to_port = 22 protocol = \u0026quot;tcp\u0026quot; cidr_blocks = flatten([\u0026quot;${values(data.aws_subnet.public_subnets).*.cidr_block}\u0026quot;, \u0026quot;${values(data.aws_subnet.private_subnets).*.cidr_block}\u0026quot;]) } #--------------------cloud-watch-log-group-- for eks cluster---------------------------------------------- resource \u0026quot;aws_cloudwatch_log_group\u0026quot; \u0026quot;eks_log_group\u0026quot; { name = \u0026quot;/aws/eks/${var.cluster_name}/cluster\u0026quot; retention_in_days = var.retention_day tags = { Name = \u0026quot;/aws/eks/${var.cluster_name}/cluster\u0026quot; vpc_id = \u0026quot;${data.aws_vpc.vpc.id}\u0026quot; ManagedBy = \u0026quot;terraform\u0026quot; Env = var.env } } #-----------------EKS-cluster-code-------------------------------------------- resource \u0026quot;aws_eks_cluster\u0026quot; \u0026quot;eks-cluster\u0026quot; { count = contains(local.env, var.env) ? 1 : 0 name = \u0026quot;${var.cluster_name}\u0026quot; enabled_cluster_log_types = [\u0026quot;api\u0026quot;,\u0026quot;audit\u0026quot;,\u0026quot;authenticator\u0026quot;,\u0026quot;controllerManager\u0026quot;,\u0026quot;scheduler\u0026quot;] version = \u0026quot;${var.eks_version}\u0026quot; role_arn = element(aws_iam_role.EKSClusterRole.*.arn,0) vpc_config { endpoint_private_access = true endpoint_public_access = true security_group_ids = [\u0026quot;${element(aws_security_group.control_plane_sg.*.id,0)}\u0026quot;] subnet_ids = flatten([\u0026quot;${values(data.aws_subnet.private_subnets).*.id}\u0026quot;]) } tags = { Name = \u0026quot;${var.cluster_name}\u0026quot; vpc_id = \u0026quot;${data.aws_vpc.vpc.id}\u0026quot; ManagedBy = \u0026quot;terraform\u0026quot; \u0026quot;karpenter.sh/discovery\u0026quot; = var.cluster_name Env = var.env } depends_on = [ aws_cloudwatch_log_group.eks_log_group, aws_iam_role_policy_attachment.eks-cluster-AmazonEKSClusterPolicy, aws_iam_role_policy_attachment.eks-cluster-AmazonEKSServicePolicy, ] } #--------------eks-private-node-group---------------------------------------------- resource \u0026quot;aws_eks_node_group\u0026quot; \u0026quot;node-group-private\u0026quot; { cluster_name = element(aws_eks_cluster.eks-cluster.*.name,0) node_group_name = var.node_group_name node_role_arn = element(aws_iam_role.NodeGroupRole.*.arn,0) subnet_ids = flatten([\u0026quot;${values(data.aws_subnet.private_subnets).*.id}\u0026quot;]) capacity_type = \u0026quot;ON_DEMAND\u0026quot; ami_type = var.ami_type disk_size = var.disk_size instance_types = var.instance_types scaling_config { desired_size = var.node_desired_size max_size = var.node_max_size min_size = var.node_min_size } lifecycle { create_before_destroy = true } timeouts {} remote_access { ec2_ssh_key = var.ec2_ssh_key_name_eks_nodes source_security_group_ids = [element(aws_security_group.eks-nodes.*.id,0)] } labels = { \u0026quot;eks/cluster-name\u0026quot; = element(aws_eks_cluster.eks-cluster.*.name,0) \u0026quot;eks/nodegroup-name\u0026quot; = format(\u0026quot;nodegroup_%s\u0026quot;, lower(element(aws_eks_cluster.eks-cluster.*.name,0))) } tags = merge({ Name = var.node_group_name \u0026quot;eks/cluster-name\u0026quot; = element(aws_eks_cluster.eks-cluster.*.name,0) \u0026quot;eks/nodegroup-name\u0026quot; = format(\u0026quot;nodegroup_%s\u0026quot;, lower(element(aws_eks_cluster.eks-cluster.*.name,0))) \u0026quot;kubernetes.io/cluster/${var.cluster_name}\u0026quot; = \u0026quot;owned\u0026quot; \u0026quot;karpenter.sh/discovery/${var.cluster_name}\u0026quot; = var.cluster_name \u0026quot;karpenter.sh/discovery\u0026quot; = var.cluster_name \u0026quot;eks/nodegroup-type\u0026quot; = \u0026quot;managed\u0026quot; vpc_id = \u0026quot;${data.aws_vpc.vpc.id}\u0026quot; ManagedBy = \u0026quot;terraform\u0026quot; Env = var.env }) # Ensure that IAM Role permissions are created before and deleted after EKS Node Group handling. # Otherwise, EKS will not be able to properly delete EC2 Instances and Elastic Network Interfaces. depends_on = [ aws_eks_cluster.eks-cluster, aws_iam_role_policy_attachment.AmazonEKSWorkerNodePolicy, aws_iam_role_policy_attachment.AmazonEKS_CNI_Policy, aws_iam_role_policy_attachment.AmazonEC2ContainerRegistryReadOnly, ] } # ADD-ONS resource \u0026quot;aws_eks_addon\u0026quot; \u0026quot;addons\u0026quot; { for_each = { for addon in var.addons : addon.name =\u0026gt; addon } cluster_name = element(aws_eks_cluster.eks-cluster.*.name,0) addon_name = each.value.name resolve_conflicts = \u0026quot;OVERWRITE\u0026quot; depends_on = [aws_eks_cluster.eks-cluster , aws_eks_node_group.node-group-private ] } IAM roles and Policies for EKS cluster and NodeGroup\n# IAM role for EKS cluster resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;EKSClusterRole\u0026quot; { count = contains(local.env, var.env) ? 1 : 0 name = \u0026quot;EKSClusterRole_v2\u0026quot; assume_role_policy = jsonencode({ Version = \u0026quot;2012-10-17\u0026quot; Statement = [ { Action = \u0026quot;sts:AssumeRole\u0026quot; Effect = \u0026quot;Allow\u0026quot; Principal = { Service = \u0026quot;eks.amazonaws.com\u0026quot; } }, ] }) } # Adding policies to the IAM role for EKS cluster resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;eks-cluster-AmazonEKSClusterPolicy\u0026quot; { policy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\u0026quot; role = element(aws_iam_role.EKSClusterRole.*.name,0) } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;eks-cluster-AmazonEKSVPCResourceController\u0026quot; { policy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEKSVPCResourceController\u0026quot; role = element(aws_iam_role.EKSClusterRole.*.name,0) } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;eks-cluster-AmazonEKSServicePolicy\u0026quot; { policy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEKSServicePolicy\u0026quot; role = element(aws_iam_role.EKSClusterRole.*.name,0) } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;eks_CloudWatchFullAccess\u0026quot; { policy_arn = \u0026quot;arn:aws:iam::aws:policy/CloudWatchFullAccess\u0026quot; role =element(aws_iam_role.EKSClusterRole.*.name,0) } ## IAM role for Node group resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;NodeGroupRole\u0026quot; { count = contains(local.env, var.env) ? 1 : 0 name = \u0026quot;EKSNodeGroupRole_v2\u0026quot; assume_role_policy = jsonencode({ Version = \u0026quot;2012-10-17\u0026quot; Statement = [ { Action = \u0026quot;sts:AssumeRole\u0026quot; Effect = \u0026quot;Allow\u0026quot; Principal = { Service = \u0026quot;ec2.amazonaws.com\u0026quot; } }, ] }) } #---policy-attachements-for-Node group-role-------- resource \u0026quot;aws_iam_role_policy\u0026quot; \u0026quot;node-group-ClusterAutoscalerPolicy\u0026quot; { name = \u0026quot;eks-cluster-auto-scaler\u0026quot; role = element(aws_iam_role.NodeGroupRole.*.id,0) policy = jsonencode({ Version = \u0026quot;2012-10-17\u0026quot; Statement = [ { Action = [ \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingInstances\u0026quot;, \u0026quot;autoscaling:DescribeLaunchConfigurations\u0026quot;, \u0026quot;autoscaling:DescribeTags\u0026quot;, \u0026quot;autoscaling:SetDesiredCapacity\u0026quot;, \u0026quot;autoscaling:TerminateInstanceInAutoScalingGroup\u0026quot; ] Effect = \u0026quot;Allow\u0026quot; Resource = \u0026quot;*\u0026quot; }, ] }) } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;node_group_AWSLoadBalancerControllerPolicy\u0026quot; { policy_arn = element(aws_iam_policy.load-balancer-policy.*.arn,0) role = element(aws_iam_role.NodeGroupRole.*.name,0) } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;AmazonEKSWorkerNodePolicy\u0026quot; { policy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\u0026quot; role = element(aws_iam_role.NodeGroupRole.*.name,0) } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;AmazonEKS_CNI_Policy\u0026quot; { policy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\u0026quot; role = element(aws_iam_role.NodeGroupRole.*.name,0) } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;AmazonEC2ContainerRegistryReadOnly\u0026quot; { policy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\u0026quot; role = element(aws_iam_role.NodeGroupRole.*.name,0) } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;CloudWatchAgentServerPolicy\u0026quot; { policy_arn = \u0026quot;arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\u0026quot; role = element(aws_iam_role.NodeGroupRole.*.name,0) } ## SSMManagedInstanceCore Policy for Nodes (Karpenter) resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;eks_node_attach_AmazonSSMManagedInstanceCore\u0026quot; { policy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\u0026quot; role = element(aws_iam_role.NodeGroupRole.*.name,0) } # Create IAM OIDC identity providers to establish trust between an OIDC-compatible IdP and your AWS account. data \u0026quot;tls_certificate\u0026quot; \u0026quot;cert\u0026quot; { url = aws_eks_cluster.eks-cluster[0].identity[0].oidc[0].issuer } resource \u0026quot;aws_iam_openid_connect_provider\u0026quot; \u0026quot;cluster\u0026quot; { client_id_list = [\u0026quot;sts.amazonaws.com\u0026quot;] thumbprint_list = [data.tls_certificate.cert.certificates[0].sha1_fingerprint] url = aws_eks_cluster.eks-cluster[0].identity[0].oidc[0].issuer } **IAM role and policies for ALB ingress controller** # IAM policy for ALB ingress controller resource \u0026quot;aws_iam_policy\u0026quot; \u0026quot;load-balancer-policy\u0026quot; { count = contains(local.env, var.env) ? 1 : 0 name = \u0026quot;AWSLoadBalancerControllerIAMPolicy_v2\u0026quot; path = \u0026quot;/\u0026quot; description = \u0026quot;AWS LoadBalancer Controller IAM Policy\u0026quot; policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;iam:CreateServiceLinkedRole\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;iam:AWSServiceName\u0026quot;: \u0026quot;elasticloadbalancing.amazonaws.com\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:DescribeAccountAttributes\u0026quot;, \u0026quot;ec2:DescribeAddresses\u0026quot;, \u0026quot;ec2:DescribeAvailabilityZones\u0026quot;, \u0026quot;ec2:DescribeInternetGateways\u0026quot;, \u0026quot;ec2:DescribeVpcs\u0026quot;, \u0026quot;ec2:DescribeVpcPeeringConnections\u0026quot;, \u0026quot;ec2:DescribeSubnets\u0026quot;, \u0026quot;ec2:DescribeSecurityGroups\u0026quot;, \u0026quot;ec2:DescribeInstances\u0026quot;, \u0026quot;ec2:DescribeNetworkInterfaces\u0026quot;, \u0026quot;ec2:DescribeTags\u0026quot;, \u0026quot;ec2:GetCoipPoolUsage\u0026quot;, \u0026quot;ec2:DescribeCoipPools\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancers\u0026quot;, \u0026quot;elasticloadbalancing:DescribeLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeListeners\u0026quot;, \u0026quot;elasticloadbalancing:DescribeListenerCertificates\u0026quot;, \u0026quot;elasticloadbalancing:DescribeSSLPolicies\u0026quot;, \u0026quot;elasticloadbalancing:DescribeRules\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTargetGroups\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTargetGroupAttributes\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTargetHealth\u0026quot;, \u0026quot;elasticloadbalancing:DescribeTags\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;cognito-idp:DescribeUserPoolClient\u0026quot;, \u0026quot;acm:ListCertificates\u0026quot;, \u0026quot;acm:DescribeCertificate\u0026quot;, \u0026quot;iam:ListServerCertificates\u0026quot;, \u0026quot;iam:GetServerCertificate\u0026quot;, \u0026quot;waf-regional:GetWebACL\u0026quot;, \u0026quot;waf-regional:GetWebACLForResource\u0026quot;, \u0026quot;waf-regional:AssociateWebACL\u0026quot;, \u0026quot;waf-regional:DisassociateWebACL\u0026quot;, \u0026quot;wafv2:GetWebACL\u0026quot;, \u0026quot;wafv2:GetWebACLForResource\u0026quot;, \u0026quot;wafv2:AssociateWebACL\u0026quot;, \u0026quot;wafv2:DisassociateWebACL\u0026quot;, \u0026quot;shield:GetSubscriptionState\u0026quot;, \u0026quot;shield:DescribeProtection\u0026quot;, \u0026quot;shield:CreateProtection\u0026quot;, \u0026quot;shield:DeleteProtection\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:AuthorizeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:RevokeSecurityGroupIngress\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:CreateSecurityGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:CreateTags\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:ec2:*:*:security-group/*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;ec2:CreateAction\u0026quot;: \u0026quot;CreateSecurityGroup\u0026quot; }, \u0026quot;Null\u0026quot;: { \u0026quot;aws:RequestTag/elbv2.k8s.aws/cluster\u0026quot;: \u0026quot;false\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;ec2:DeleteTags\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:ec2:*:*:security-group/*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;Null\u0026quot;: { \u0026quot;aws:RequestTag/elbv2.k8s.aws/cluster\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;aws:ResourceTag/elbv2.k8s.aws/cluster\u0026quot;: \u0026quot;false\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:AuthorizeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:RevokeSecurityGroupIngress\u0026quot;, \u0026quot;ec2:DeleteSecurityGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;Null\u0026quot;: { \u0026quot;aws:ResourceTag/elbv2.k8s.aws/cluster\u0026quot;: \u0026quot;false\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;elasticloadbalancing:CreateLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:CreateTargetGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;Null\u0026quot;: { \u0026quot;aws:RequestTag/elbv2.k8s.aws/cluster\u0026quot;: \u0026quot;false\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;elasticloadbalancing:CreateListener\u0026quot;, \u0026quot;elasticloadbalancing:DeleteListener\u0026quot;, \u0026quot;elasticloadbalancing:CreateRule\u0026quot;, \u0026quot;elasticloadbalancing:DeleteRule\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;elasticloadbalancing:AddTags\u0026quot;, \u0026quot;elasticloadbalancing:RemoveTags\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:elasticloadbalancing:*:*:targetgroup/*/*\u0026quot;, \u0026quot;arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*\u0026quot;, \u0026quot;arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*\u0026quot; ], \u0026quot;Condition\u0026quot;: { \u0026quot;Null\u0026quot;: { \u0026quot;aws:RequestTag/elbv2.k8s.aws/cluster\u0026quot;: \u0026quot;true\u0026quot;, \u0026quot;aws:ResourceTag/elbv2.k8s.aws/cluster\u0026quot;: \u0026quot;false\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;elasticloadbalancing:AddTags\u0026quot;, \u0026quot;elasticloadbalancing:RemoveTags\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:elasticloadbalancing:*:*:listener/net/*/*/*\u0026quot;, \u0026quot;arn:aws:elasticloadbalancing:*:*:listener/app/*/*/*\u0026quot;, \u0026quot;arn:aws:elasticloadbalancing:*:*:listener-rule/net/*/*/*\u0026quot;, \u0026quot;arn:aws:elasticloadbalancing:*:*:listener-rule/app/*/*/*\u0026quot; ] }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;elasticloadbalancing:ModifyLoadBalancerAttributes\u0026quot;, \u0026quot;elasticloadbalancing:SetIpAddressType\u0026quot;, \u0026quot;elasticloadbalancing:SetSecurityGroups\u0026quot;, \u0026quot;elasticloadbalancing:SetSubnets\u0026quot;, \u0026quot;elasticloadbalancing:DeleteLoadBalancer\u0026quot;, \u0026quot;elasticloadbalancing:ModifyTargetGroup\u0026quot;, \u0026quot;elasticloadbalancing:ModifyTargetGroupAttributes\u0026quot;, \u0026quot;elasticloadbalancing:DeleteTargetGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;Null\u0026quot;: { \u0026quot;aws:ResourceTag/elbv2.k8s.aws/cluster\u0026quot;: \u0026quot;false\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;elasticloadbalancing:RegisterTargets\u0026quot;, \u0026quot;elasticloadbalancing:DeregisterTargets\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:elasticloadbalancing:*:*:targetgroup/*/*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;elasticloadbalancing:SetWebAcl\u0026quot;, \u0026quot;elasticloadbalancing:ModifyListener\u0026quot;, \u0026quot;elasticloadbalancing:AddListenerCertificates\u0026quot;, \u0026quot;elasticloadbalancing:RemoveListenerCertificates\u0026quot;, \u0026quot;elasticloadbalancing:ModifyRule\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EOF } # IAM role and policy document for ALB ingress controller data \u0026quot;aws_iam_policy_document\u0026quot; \u0026quot;eks_oidc_assume_role\u0026quot; { statement { actions = [\u0026quot;sts:AssumeRoleWithWebIdentity\u0026quot;] effect = \u0026quot;Allow\u0026quot; condition { test = \u0026quot;StringEquals\u0026quot; variable = \u0026quot;${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, \u0026quot;https://\u0026quot;, \u0026quot;\u0026quot;)}:sub\u0026quot; values = [ \u0026quot;system:serviceaccount:kube-system:aws-load-balancer-controller\u0026quot; ] } condition { test = \u0026quot;StringEquals\u0026quot; variable = \u0026quot;${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, \u0026quot;https://\u0026quot;, \u0026quot;\u0026quot;)}:aud\u0026quot; values = [ \u0026quot;sts.amazonaws.com\u0026quot; ] } principals { identifiers = [ \u0026quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, \u0026quot;https://\u0026quot;, \u0026quot;\u0026quot;)}\u0026quot; ] type = \u0026quot;Federated\u0026quot; } } } resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;alb_ingress\u0026quot; { count = contains(local.env, var.env) ? 1 : 0 name = \u0026quot;${var.cluster_name}-alb-ingress\u0026quot; assume_role_policy = data.aws_iam_policy_document.eks_oidc_assume_role.json } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;load-balancer-policy-role\u0026quot; { policy_arn = element(aws_iam_policy.load-balancer-policy.*.arn,0) role = element(aws_iam_role.alb_ingress.*.name,0) } IAM roles and policies for EBS CSI driver\ndata \u0026quot;aws_iam_policy_document\u0026quot; \u0026quot;eks_oidc_assume_role_ebs\u0026quot; { statement { actions = [\u0026quot;sts:AssumeRoleWithWebIdentity\u0026quot;] effect = \u0026quot;Allow\u0026quot; condition { test = \u0026quot;StringEquals\u0026quot; variable = \u0026quot;${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, \u0026quot;https://\u0026quot;, \u0026quot;\u0026quot;)}:sub\u0026quot; values = [ \u0026quot;system:serviceaccount:kube-system:ebs-csi-controller-sa\u0026quot; ] } principals { identifiers = [ \u0026quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, \u0026quot;https://\u0026quot;, \u0026quot;\u0026quot;)}\u0026quot; ] type = \u0026quot;Federated\u0026quot; } } } resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;ebs-csi\u0026quot; { name = \u0026quot;ebs-csi-role\u0026quot; assume_role_policy = data.aws_iam_policy_document.eks_oidc_assume_role_ebs.json } # IAM policy for EBS CSI controller resource \u0026quot;aws_iam_policy\u0026quot; \u0026quot;ebs_permissions\u0026quot; { name = \u0026quot;ebs-permissions\u0026quot; path = \u0026quot;/\u0026quot; description = \u0026quot;AWS EBS Controller IAM Policy\u0026quot; policy = \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:CreateSnapshot\u0026quot;, \u0026quot;ec2:AttachVolume\u0026quot;, \u0026quot;ec2:DetachVolume\u0026quot;, \u0026quot;ec2:ModifyVolume\u0026quot;, \u0026quot;ec2:DescribeAvailabilityZones\u0026quot;, \u0026quot;ec2:DescribeInstances\u0026quot;, \u0026quot;ec2:DescribeSnapshots\u0026quot;, \u0026quot;ec2:DescribeTags\u0026quot;, \u0026quot;ec2:DescribeVolumes\u0026quot;, \u0026quot;ec2:DescribeVolumesModifications\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:CreateTags\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:ec2:*:*:volume/*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:snapshot/*\u0026quot; ], \u0026quot;Condition\u0026quot;: { \u0026quot;StringEquals\u0026quot;: { \u0026quot;ec2:CreateAction\u0026quot;: [ \u0026quot;CreateVolume\u0026quot;, \u0026quot;CreateSnapshot\u0026quot; ] } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:DeleteTags\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:ec2:*:*:volume/*\u0026quot;, \u0026quot;arn:aws:ec2:*:*:snapshot/*\u0026quot; ] }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:CreateVolume\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;aws:RequestTag/ebs.csi.aws.com/cluster\u0026quot;: \u0026quot;true\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:CreateVolume\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;aws:RequestTag/CSIVolumeName\u0026quot;: \u0026quot;*\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:DeleteVolume\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;ec2:ResourceTag/CSIVolumeName\u0026quot;: \u0026quot;*\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:DeleteVolume\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;ec2:ResourceTag/ebs.csi.aws.com/cluster\u0026quot;: \u0026quot;true\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:DeleteSnapshot\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;ec2:ResourceTag/CSIVolumeSnapshotName\u0026quot;: \u0026quot;*\u0026quot; } } }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:DeleteSnapshot\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Condition\u0026quot;: { \u0026quot;StringLike\u0026quot;: { \u0026quot;ec2:ResourceTag/ebs.csi.aws.com/cluster\u0026quot;: \u0026quot;true\u0026quot; } } } ] } EOF } resource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;ebs-csi-policy-role\u0026quot; { policy_arn = aws_iam_policy.ebs_permissions.arn role = aws_iam_role.ebs-csi.name } Template Files ( Custom values.yml for helm chart ) FOR ALB-INGRESS AND EBS-CSI DRIVER\ndata \u0026quot;template_file\u0026quot; \u0026quot;alb-ingress-values\u0026quot; { template = \u0026lt;\u0026lt;EOF replicaCount: 1vpcId: \u0026quot;${data.aws_vpc.vpc.id}\u0026quot; clusterName: \u0026quot;${var.cluster_name}\u0026quot; ingressClass: alb createIngressClassResource: true region: \u0026quot;${var.region}\u0026quot; resources: requests: memory: 256Mi cpu: 100m limits: memory: 512Mi cpu: 1000m EOF } data \u0026quot;template_file\u0026quot; \u0026quot;ebs-csi-driver-values\u0026quot; { template = \u0026lt;\u0026lt;EOF controller: region: \u0026quot;${var.region}\u0026quot; replicaCount: 1 k8sTagClusterId: \u0026quot;${var.cluster_name}\u0026quot; updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 resources: requests: cpu: 10m memory: 40Mi limits: cpu: 100m memory: 256Mi serviceAccount: create: true name: ebs-csi-controller-sa annotations: eks.amazonaws.com/role-arn: \u0026quot;${aws_iam_role.ebs-csi.arn}\u0026quot; storageClasses: - name: ebs-sc annotations: storageclass.kubernetes.io/is-default-class: \u0026quot;true\u0026quot; EOF } Kuberenets Resources for ALB ingress controller resource \u0026quot;kubernetes_service_account\u0026quot; \u0026quot;aws-load-balancer-controller-service-account\u0026quot; { metadata { name = \u0026quot;aws-load-balancer-controller\u0026quot; namespace = \u0026quot;kube-system\u0026quot; annotations = { \u0026quot;eks.amazonaws.com/role-arn\u0026quot; = data.aws_iam_role.alb_ingress.arn } labels = { \u0026quot;app.kubernetes.io/name\u0026quot; = \u0026quot;aws-load-balancer-controller\u0026quot; \u0026quot;app.kubernetes.io/component\u0026quot; = \u0026quot;controller\u0026quot; \u0026quot;app.kubernetes.io/managed-by\u0026quot; = \u0026quot;terraform\u0026quot; } } automount_service_account_token = true depends_on = [aws_eks_cluster.eks-cluster , aws_eks_node_group.node-group-private ] } resource \u0026quot;kubernetes_secret\u0026quot; \u0026quot;aws-load-balancer-controller\u0026quot; { metadata { name = \u0026quot;aws-load-balancer-controller\u0026quot; namespace = \u0026quot;kube-system\u0026quot; annotations = { \u0026quot;kubernetes.io/service-account.name\u0026quot; = \u0026quot;aws-load-balancer-controller\u0026quot; \u0026quot;kubernetes.io/service-account.namespace\u0026quot; = \u0026quot;kube-system\u0026quot; } } type = \u0026quot;kubernetes.io/service-account-token\u0026quot; depends_on = [kubernetes_service_account.aws-load-balancer-controller-service-account , aws_eks_cluster.eks-cluster , aws_eks_node_group.node-group-private] } resource \u0026quot;kubernetes_cluster_role\u0026quot; \u0026quot;aws-load-balancer-controller-cluster-role\u0026quot; { depends_on = [aws_eks_cluster.eks-cluster , aws_eks_node_group.node-group-private ] metadata { name = \u0026quot;aws-load-balancer-controller\u0026quot; labels = { \u0026quot;app.kubernetes.io/name\u0026quot; = \u0026quot;aws-load-balancer-controller\u0026quot; \u0026quot;app.kubernetes.io/managed-by\u0026quot; = \u0026quot;terraform\u0026quot; } } rule { api_groups = [ \u0026quot;\u0026quot;, \u0026quot;extensions\u0026quot;, ] resources = [ \u0026quot;configmaps\u0026quot;, \u0026quot;endpoints\u0026quot;, \u0026quot;events\u0026quot;, \u0026quot;ingresses\u0026quot;, \u0026quot;ingresses/status\u0026quot;, \u0026quot;services\u0026quot;, ] verbs = [ \u0026quot;create\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;patch\u0026quot;, ] } rule { api_groups = [ \u0026quot;\u0026quot;, \u0026quot;extensions\u0026quot;, ] resources = [ \u0026quot;nodes\u0026quot;, \u0026quot;pods\u0026quot;, \u0026quot;secrets\u0026quot;, \u0026quot;services\u0026quot;, \u0026quot;namespaces\u0026quot;, ] verbs = [ \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, ] } } resource \u0026quot;kubernetes_cluster_role_binding\u0026quot; \u0026quot;aws-load-balancer-controller-cluster-role-binding\u0026quot; { depends_on = [aws_eks_cluster.eks-cluster , aws_eks_node_group.node-group-private ] metadata { name = \u0026quot;aws-load-balancer-controller\u0026quot; labels = { \u0026quot;app.kubernetes.io/name\u0026quot; = \u0026quot;aws-load-balancer-controller\u0026quot; \u0026quot;app.kubernetes.io/managed-by\u0026quot; = \u0026quot;terraform\u0026quot; } } role_ref { api_group = \u0026quot;rbac.authorization.k8s.io\u0026quot; kind = \u0026quot;ClusterRole\u0026quot; name = kubernetes_cluster_role.aws-load-balancer-controller-cluster-role.metadata[0].name } subject { api_group = \u0026quot;\u0026quot; kind = \u0026quot;ServiceAccount\u0026quot; name = kubernetes_service_account.aws-load-balancer-controller-service-account.metadata[0].name namespace = kubernetes_service_account.aws-load-balancer-controller-service-account.metadata[0].namespace } } Helm Release for ALB ingress controller and EBS CSI driver and Metrics Server\nresource \u0026quot;helm_release\u0026quot; \u0026quot;alb-ingress-controller\u0026quot; { depends_on = [ aws_eks_cluster.eks-cluster , aws_eks_node_group.node-group-private , kubernetes_cluster_role_binding.aws-load-balancer-controller-cluster-role-binding, kubernetes_service_account.aws-load-balancer-controller-service-account , kubernetes_secret.aws-load-balancer-controller , helm_release.karpenter , kubectl_manifest.karpenter-provisioner ] name = \u0026quot;alb-ingress-controller\u0026quot; repository = \u0026quot;\u0026lt;https://aws.github.io/eks-charts\u0026gt;\u0026quot; version = \u0026quot;1.4.7\u0026quot; chart = \u0026quot;aws-load-balancer-controller\u0026quot; namespace = \u0026quot;kube-system\u0026quot; values = [data.template_file.alb-ingress-values.rendered] set { name = \u0026quot;serviceAccount.create\u0026quot; value = \u0026quot;false\u0026quot; } set { name = \u0026quot;serviceAccount.name\u0026quot; value = \u0026quot;${kubernetes_service_account.aws-load-balancer-controller-service-account.metadata[0].name}\u0026quot; } } resource \u0026quot;helm_release\u0026quot; \u0026quot;aws-ebs-csi-driver\u0026quot; { depends_on = [ aws_eks_cluster.eks-cluster , aws_eks_node_group.node-group-private , helm_release.karpenter , kubectl_manifest.karpenter-provisioner , aws_iam_role.ebs-csi ] name = \u0026quot;aws-ebs-csi-driver\u0026quot; repository = \u0026quot;\u0026lt;https://kubernetes-sigs.github.io/aws-ebs-csi-driver\u0026gt;\u0026quot; chart = \u0026quot;aws-ebs-csi-driver\u0026quot; namespace = \u0026quot;kube-system\u0026quot; create_namespace = true values = [data.template_file.ebs-csi-driver-values.rendered] } resource \u0026quot;helm_release\u0026quot; \u0026quot;metrics-server\u0026quot; { depends_on = [helm_release.karpenter , kubectl_manifest.karpenter-provisioner ] name = \u0026quot;metrics-server\u0026quot; chart = \u0026quot;metrics-server\u0026quot; repository = \u0026quot;\u0026lt;https://kubernetes-sigs.github.io/metrics-server/\u0026gt;\u0026quot; version = \u0026quot;3.8.2\u0026quot; namespace = \u0026quot;kube-system\u0026quot; description = \u0026quot;Metric server helm Chart deployment configuration\u0026quot; } IAM role and policies for Karpenter controller\ndata \u0026quot;aws_iam_policy_document\u0026quot; \u0026quot;karpenter_controller_assume_role_policy\u0026quot; { statement { actions = [\u0026quot;sts:AssumeRoleWithWebIdentity\u0026quot;] effect = \u0026quot;Allow\u0026quot; condition { test = \u0026quot;StringEquals\u0026quot; variable = \u0026quot;${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, \u0026quot;https://\u0026quot;, \u0026quot;\u0026quot;)}:sub\u0026quot; values = [\u0026quot;system:serviceaccount:karpenter:karpenter\u0026quot;] } condition { test = \u0026quot;StringEquals\u0026quot; variable = \u0026quot;${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, \u0026quot;https://\u0026quot;, \u0026quot;\u0026quot;)}:aud\u0026quot; values = [\u0026quot;sts.amazonaws.com\u0026quot;] } principals { identifiers = [ \u0026quot;arn:aws:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${replace(data.aws_eks_cluster.cluster.identity[0].oidc[0].issuer, \u0026quot;https://\u0026quot;, \u0026quot;\u0026quot;)}\u0026quot; ] type = \u0026quot;Federated\u0026quot; } } } data \u0026quot;aws_iam_policy_document\u0026quot; \u0026quot;karpenter\u0026quot; { statement { resources = [\u0026quot;*\u0026quot;] actions = [\u0026quot;ec2:DescribeImages\u0026quot;, \u0026quot;ec2:RunInstances\u0026quot;, \u0026quot;ec2:DescribeSubnets\u0026quot;, \u0026quot;ec2:DescribeSecurityGroups\u0026quot;, \u0026quot;ec2:DescribeLaunchTemplates\u0026quot;, \u0026quot;ec2:DescribeInstances\u0026quot;, \u0026quot;ec2:DescribeInstanceTypes\u0026quot;, \u0026quot;ec2:DescribeInstanceTypeOfferings\u0026quot;, \u0026quot;ec2:DescribeAvailabilityZones\u0026quot;, \u0026quot;ec2:DeleteLaunchTemplate\u0026quot;, \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;ec2:CreateLaunchTemplate\u0026quot;, \u0026quot;ec2:CreateFleet\u0026quot;, \u0026quot;ec2:DescribeSpotPriceHistory\u0026quot;, \u0026quot;pricing:GetProducts\u0026quot;, \u0026quot;ssm:GetParameter\u0026quot;] effect = \u0026quot;Allow\u0026quot; } statement { resources = [\u0026quot;*\u0026quot;] actions = [\u0026quot;ec2:TerminateInstances\u0026quot;, \u0026quot;ec2:DeleteLaunchTemplate\u0026quot; , \u0026quot;ec2:RequestSpotInstances\u0026quot; , \u0026quot;ec2:DescribeInstanceStatus\u0026quot; , \u0026quot;iam:CreateServiceLinkedRole\u0026quot; , \u0026quot;iam:ListRoles\u0026quot; , \u0026quot;iam:ListInstanceProfiles\u0026quot;] effect = \u0026quot;Allow\u0026quot; # Make sure Karpenter can only delete nodes that it has provisioned condition { test = \u0026quot;StringEquals\u0026quot; values = [var.eks_cluster_name] variable = \u0026quot;ec2:ResourceTag/karpenter.sh/discovery\u0026quot; } } statement { resources = [data.aws_eks_cluster.cluster.arn] actions = [\u0026quot;eks:DescribeCluster\u0026quot;] effect = \u0026quot;Allow\u0026quot; } statement { resources = [element(aws_iam_role.NodeGroupRole.*.arn,0)] actions = [\u0026quot;iam:PassRole\u0026quot;] effect = \u0026quot;Allow\u0026quot; } # Optional: Interrupt Termination Queue permissions, provided by AWS SQS statement { resources = [aws_sqs_queue.karpenter.arn] actions = [\u0026quot;sqs:DeleteMessage\u0026quot;, \u0026quot;sqs:GetQueueUrl\u0026quot;, \u0026quot;sqs:GetQueueAttributes\u0026quot;, \u0026quot;sqs:ReceiveMessage\u0026quot;] effect = \u0026quot;Allow\u0026quot; } } resource \u0026quot;aws_iam_role\u0026quot; \u0026quot;karpenter_controller\u0026quot; { description = \u0026quot;IAM Role for Karpenter Controller (pod) to assume\u0026quot; assume_role_policy = data.aws_iam_policy_document.karpenter_controller_assume_role_policy.json name = \u0026quot;karpenter-controller\u0026quot; inline_policy { policy = data.aws_iam_policy_document.karpenter.json name = \u0026quot;karpenter\u0026quot; } depends_on = [data.aws_iam_policy_document.karpenter_controller_assume_role_policy , data.aws_iam_policy_document.karpenter] } Instance Profile for Karpenter ## Karpenter Instance Profile resource \u0026quot;aws_iam_instance_profile\u0026quot; \u0026quot;karpenter\u0026quot; { name = \u0026quot;karpenter-instance-profile\u0026quot; role = element(aws_iam_role.NodeGroupRole.*.name,0) depends_on = [ aws_iam_role.NodeGroupRole ] } Handling Interruption For Spot Instance via SQS queue and Event Bridge rules # SQS Queue resource \u0026quot;aws_sqs_queue\u0026quot; \u0026quot;karpenter\u0026quot; { message_retention_seconds = 300 name = \u0026quot;${var.cluster_name}-karpenter-sqs-queue\u0026quot; } # Node termination queue policy resource \u0026quot;aws_sqs_queue_policy\u0026quot; \u0026quot;karpenter\u0026quot; { policy = data.aws_iam_policy_document.node_termination_queue.json queue_url = aws_sqs_queue.karpenter.url } data \u0026quot;aws_iam_policy_document\u0026quot; \u0026quot;node_termination_queue\u0026quot; { statement { resources = [aws_sqs_queue.karpenter.arn] sid = \u0026quot;SQSWrite\u0026quot; actions = [\u0026quot;sqs:SendMessage\u0026quot;] principals { type = \u0026quot;Service\u0026quot; identifiers = [\u0026quot;events.amazonaws.com\u0026quot;, \u0026quot;sqs.amazonaws.com\u0026quot;] } } } resource \u0026quot;aws_cloudwatch_event_rule\u0026quot; \u0026quot;scheduled_change_rule\u0026quot; { name = \u0026quot;ScheduledChangeRule\u0026quot; description = \u0026quot;AWS Health Event\u0026quot; event_pattern = jsonencode({ source = [\u0026quot;aws.health\u0026quot;] detail_type = [\u0026quot;AWS Health Event\u0026quot;] }) } resource \u0026quot;aws_cloudwatch_event_rule\u0026quot; \u0026quot;spot_interruption_rule\u0026quot; { name = \u0026quot;SpotInterruptionRule\u0026quot; description = \u0026quot;EC2 Spot Instance Interruption Warning\u0026quot; event_pattern = jsonencode({ source = [\u0026quot;aws.ec2\u0026quot;] detail_type = [\u0026quot;EC2 Spot Instance Interruption Warning\u0026quot;] }) } resource \u0026quot;aws_cloudwatch_event_rule\u0026quot; \u0026quot;rebalance_rule\u0026quot; { name = \u0026quot;RebalanceRule\u0026quot; description = \u0026quot;EC2 Instance Rebalance Recommendation\u0026quot; event_pattern = jsonencode({ source = [\u0026quot;aws.ec2\u0026quot;] detail_type = [\u0026quot;EC2 Instance Rebalance Recommendation\u0026quot;] }) } resource \u0026quot;aws_cloudwatch_event_rule\u0026quot; \u0026quot;instance_state_change_rule\u0026quot; { name = \u0026quot;InstanceStateChangeRule\u0026quot; description = \u0026quot;EC2 Instance State-change Notification\u0026quot; event_pattern = jsonencode({ source = [\u0026quot;aws.ec2\u0026quot;] detail_type = [\u0026quot;EC2 Instance State-change Notification\u0026quot;] }) } resource \u0026quot;aws_cloudwatch_event_target\u0026quot; \u0026quot;scheduled_change_rule\u0026quot; { rule = aws_cloudwatch_event_rule.scheduled_change_rule.name arn = aws_sqs_queue.karpenter.arn } resource \u0026quot;aws_cloudwatch_event_target\u0026quot; \u0026quot;spot_interruption_rule\u0026quot; { rule = aws_cloudwatch_event_rule.spot_interruption_rule.name arn = aws_sqs_queue.karpenter.arn } resource \u0026quot;aws_cloudwatch_event_target\u0026quot; \u0026quot;rebalance_rule\u0026quot; { rule = aws_cloudwatch_event_rule.rebalance_rule.name arn = aws_sqs_queue.karpenter.arn } resource \u0026quot;aws_cloudwatch_event_target\u0026quot; \u0026quot;instance_state_change_rule\u0026quot; { rule = aws_cloudwatch_event_rule.instance_state_change_rule.name arn = aws_sqs_queue.karpenter.arn } Template File ( values.yml ) for Karpenter data \u0026quot;template_file\u0026quot; \u0026quot;karpenter\u0026quot; { template = \u0026lt;\u0026lt;EOF serviceAccount: annotations: eks.amazonaws.com/role-arn: \u0026quot;${aws_iam_role.karpenter_controller.arn}\u0026quot; settings: aws: clusterName: \u0026quot;${data.aws_eks_cluster.cluster.id}\u0026quot; clusterEndpoint: \u0026quot;${data.aws_eks_cluster.cluster.endpoint}\u0026quot; defaultInstanceProfile: \u0026quot;${aws_iam_instance_profile.karpenter.name}\u0026quot; interruptionQueueName: \u0026quot;${var.cluster_name}-karpenter-sqs-queue\u0026quot; EOF } Helm release for Karpenter resource \u0026quot;helm_release\u0026quot; \u0026quot;karpenter\u0026quot; { namespace = \u0026quot;karpenter\u0026quot; create_namespace = true name = \u0026quot;karpenter\u0026quot; repository = \u0026quot;oci://public.ecr.aws/karpenter\u0026quot; repository_username = data.aws_ecrpublic_authorization_token.token.user_name repository_password = data.aws_ecrpublic_authorization_token.token.password chart = \u0026quot;karpenter\u0026quot; version = \u0026quot;v0.27.0\u0026quot; values = [data.template_file.karpenter.rendered] # set { # name = \u0026quot;serviceAccount.annotations.eks\\\\.amazonaws\\\\.com/role-arn\u0026quot; # value = \u0026quot;${aws_iam_role.karpenter_controller.arn}\u0026quot; # } # set { # name = \u0026quot;settings.aws.clusterName\u0026quot; # value = data.aws_eks_cluster.cluster.id # } # set { # name = \u0026quot;settings.aws.clusterEndpoint\u0026quot; # value = data.aws_eks_cluster.cluster.endpoint # } # set { # name = \u0026quot;settings.aws.defaultInstanceProfile\u0026quot; # value = aws_iam_instance_profile.karpenter.name # } # set { # name = \u0026quot;settings.aws.interruptionQueueName\u0026quot; # value = \u0026quot;${var.cluster_name}-karpenter-sqs-queue\u0026quot; # } depends_on = [ aws_eks_cluster.eks-cluster , aws_eks_node_group.node-group-private , aws_iam_role.karpenter_controller , aws_iam_instance_profile.karpenter , aws_sqs_queue.karpenter ] } Not finished yet !! We need Provisioners installed which decides what type of instance to bring up based on the workload.\nProvisioners are CRDs that Karpenter uses to provision new nodes. New nodes are brought up based on the pods that are waiting to be scheduled and their scheduling constraints\nConfiguring Default Karpenter Provisioner\nresource \u0026quot;kubectl_manifest\u0026quot; \u0026quot;karpenter-provisioner\u0026quot; { depends_on = [ helm_release.karpenter , aws_eks_cluster.eks-cluster , aws_eks_node_group.node-group-private] yaml_body = \u0026lt;\u0026lt;YAML apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: karpenter-default namespace: karpenter spec: provider: securityGroupSelector: karpenter.sh/discovery: \u0026quot;${var.cluster_name}\u0026quot; subnetSelector: karpenter.sh/discovery: \u0026quot;${var.cluster_name}\u0026quot; tags: karpenter.sh/discovery: \u0026quot;${var.cluster_name}\u0026quot; requirements: - key: karpenter.sh/capacity-type operator: In values: [\u0026quot;spot\u0026quot;] - key: \u0026quot;node.kubernetes.io/instance-type\u0026quot; operator: In values: [\u0026quot;t3a.micro\u0026quot; , \u0026quot;t3a.medium\u0026quot; , \u0026quot;t3a.large\u0026quot; ] - key: \u0026quot;topology.kubernetes.io/zone\u0026quot; operator: In values: [\u0026quot;us-east-1a\u0026quot;, \u0026quot;us-east-1b\u0026quot; , \u0026quot;us-east-1c\u0026quot;] - key: created-by operator: In values: [\u0026quot;karpenter\u0026quot;] ttlSecondsAfterEmpty: 30 YAML } Note: securityGroupSelector and subnetSelector ‚Äî These are used by Karpenter to identify which subnets and security groups should be used for the new nodes. When you bring up your EKS cluster you should add the respective tags to your private_subnets and to your node_security_group. Not setting these correctly can cause Karpenter to not bring up nodes or the nodes that are brought up can have difficulties trying to join the cluster.\nThe constraints ttlSecondsUntilExpired defines the node expiry so a newer node will be provisioned, and ttlSecondsAfterEmpty defines when to delete a node since the last workload stops running. (Note: DaemonSets are not taken into account.)\nDeploying via Terraform Cloud !!! I am deploying the EKS Cluster to the AWS via Terraform Cloud . I have workspaces and made connections to the repository and updated variables.\nWorkspace Run Overview:\nWorkspace Variables Overview:\nWorkspace Planned:\nWorkspace Applied:\nTesting it !! Its Time for Action Updating the Kubeconfig File\naws eks update-kubeconfig --region us-east-1 --name karpenter-demo Listing all Pods in the Cluster\nkubectl get pods -A Listing all resources in Karpenter Namespace\nkubectl create deployment nginx --image=nginx kubectl set resources deployment nginx --requests=cpu=100m,memory=256Mi kubectl scale deployment nginx --replicas 12 We can see a new node got provisioned . To verify that it created a spot instance lets go to the console and see:\nNow lets scale down to see what happens.\nkubectl scale deployment nginx --replicas 1 Immediately after that :\nLets check console whats happening.\nThis confirms our setup was successfully completed !!\nThank you for reading my article , have a nice day and keep learning!\n","permalink":"https://nithin-john-george.github.io/blog/karpenter-on-eks-with-spot/","summary":"üîó Medium In this blog you will learn how to provision EKS Kubernetes cluster at any scale using Karpenter . We will also utilize **spot instances **to reduce costs by up to 90 percent. Additionally, we‚Äôll set up the **AWS ALB Ingress Controller **and Amazon EBS CSI driver, establish trust between an OIDC-compatible identity provider and your AWS account using IAM OIDC identity provider (including the necessary IAM roles and policies), install the metrics server, and set up required EKS add-ons using Terraform.","title":"Karpenter : Setting Up On EKS Cluster (v1.24) with Spot-Instances"},{"content":"üîó Medium In this blog, we will discover how Terraform Cloud can help you streamline your cloud infrastructure management process and we will explore the powerful features that make it a must-have tool for cloud infrastructure management.\nWe will deliver the reasons why Terraform Cloud is essential, highlighting its capabilities and benefits, advanced workflows, and much more. We‚Äôll take a closer look at the VCS-driven workflow, providing a detailed example to illustrate how it works in practice.\nPlus, we‚Äôll provide some practical recommendations for using Terraform Cloud in a production environment.\nOne of the main benefits of using Terraform to manage cloud infrastructure is that it allows for infrastructure automation. With Terraform, developers and operations teams can define and manage their infrastructure in code, rather than manually configure resources through a web console or command line interface. This makes it easier to manage large, complex environments and reduces the risk of human error.\nEarlier, when using Terraform, the typical workflow involved developers defining their infrastructure as code using the Terraform configuration language(HCL), and then using the terraform plan and terraform apply commands to provision the infrastructure on their local machines.\nThe state of the infrastructure, which tracks the current state of the resources managed by Terraform, was often stored locally as well. However, as our cloud infrastructure grew, we started seeing inconsistencies and issues in the terraform state file as many team members were deploying resources at the same time.\nThat‚Äôs where Terraform cloud came in as a savior.\nTerraform cloud is a managed solution offered by Hashicorp to run Terraform configurations.\nTerraform Cloud is available as a hosted service at **https://app.terraform.io**. It acts as a CI/CD tool for deploying Infrastructure as Code (IaC)\nIt offers a central UI for all your project and provides remote state management also it manages Terraform runs ( plan / apply ) in a consistent and reliable environment, access controls for approving changes to infrastructure, a private registry for sharing Terraform modules, Cost Estimation, Drift Detection, Sentinel for Governance (policy controls for governing the contents of Terraform configurations), and more.\nYou can use it to manage cloud infrastructure, including Amazon Web Services, Google Cloud Platform, and Microsoft Azure. It‚Äôs a recommended product if you needed to manage resources within an enterprise and you have a whole team contributing to it.\nWhy use Terraform Cloud? Remote execution: It allows you to run Terraform plan and apply operations on infrastructure hosted in the cloud (TFC) or on a central system that you can control (Terraform Cloud Enterprise)\n2. Terraform cloud workspaces:\nWorkspaces are simply referring to environments. It works like separate working directories.\nIf we use multiple environments like development, QA, and production, we need to manage independent codes for each and every environment.\nBut with the help of Workspaces, we can use a single code for multiple environments and manage the State files of all the environments independently. It helps to create multiple State files with the same Terraform configurations.\nTerraform cloud manages infrastructure collections with workspaces. It works like separate working directories. Cloud workspaces are not limited to state files, configurations, variables, state management \u0026amp; credentials \u0026amp; secrets.\nIn addition to the basic Terraform content, Terraform Cloud keeps some additional data for each workspace:\nState versions: Each workspace retains backups of its previous state files. Although only the current state is necessary for managing resources, the state history can be useful for tracking changes over time or recovering from problems.\nRun History: When Terraform Cloud manages a workspace‚Äôs Terraform runs, it retains a record of all run activity, including summaries, logs, a reference to the changes that caused the run, and user comments.\n3. Version control integration and triggers:\nAs Terraform Cloud is an infrastructure-as-code service, It supports Version control system integration with some of VCS providers like GitHub, Gitlab, Azure DevOps, and Bitbucket. Terraform executions can be triggered upon new commits merged by pull requests or pushed in VCS. It can work as a CICD pipeline.\n4. Private registry:\nReusable modules are very useful when needing to provision multiple environments from the same source configuration when the requirement is to store that module privately in a Terraform cloud. The private registry can be used in this scenario. A private registry allows us to store modules and use those modules in workspaces.\n5. Access Control \u0026amp; Governance:\nLarge teams in organizations have many workspaces in Terraform cloud, And to manage access to those workspaces across team members is required most, Terraform cloud provides paid functionality to meet the control and governance needs of large organizations.\n6. Sentinel policies \u0026amp; Cost estimation:\nTerraform cloud provides sentinel, which is a policy-as-code framework. Which is used to enforce granular policies about how the infrastructure should get provisioned. Examples like limiting the size of virtual machines, confining major updates to defined maintenance windows, etc. These policies can act as firm requirements, advisory warnings, or soft requirements, which can be bypassed through explicit approval from the team.\n7. Drift Detection:\nIt continuously checks against the infrastructure state to detect changes and provide alerts. This allows Terraform to provide a central pane of visibility to all your infrastructure and offers operators confidence that it matches the last known Terraform state.\n8. Cost Optimization:\nCost Optimization is a unique feature of Terraform cloud. Before making changes to the provider, Terraform cloud can display an estimate of its total cost, as well as any change in cost caused by proposed updates.\nCost optimization can also be integrated with sentinel policies for warnings. It ensures that all your infrastructure is governed and has the proper security measures in place. It also reduces the potential for application downtime that could negatively impact the user experience and, eventually, revenue.\nHow does it work? Terraform Cloud uses organizations and workspaces to organize your code and environments.\n‚ÄúOrganizations are shared spaces for teams to collaborate on infrastructure‚Äù.\nEach organization can have multiple workspaces that define groups of resources of your infrastructure.\nNormally, a user will create one organization (common use case for small and medium companies) and have multiple workspaces describing your environments\nOrganizing Workspaces with Projects\nA project is a container that holds multiple workspaces. Projects help to organize workspaces and one key benefit of using projects is that they allow for more granular control over permissions. Each project has separate permission set that you can use to grant teams access to all workspaces in the project.\nEvery workspace must belong to exactly one project. By default, all workspaces belong to an organization‚Äôs Default Project.\nEnough of theory, Let‚Äôs start with the Practicals: First, we will create the Organization itself: Next, We will be Connecting to a version control provider Connecting Terraform Cloud to your VCS involves four steps:\nGo to your organization‚Äôs settings and then click Providers. The VCS Providers page appears. Click Add VCS Provider. The VCS Providers page appears.\nSelect GitHub and then select **GitHub.com** from the menu.\nThis will take us to the page where have to configure OAuth authentication with your VCS provider.\nNext thing on Github, we have to create a New OAuth Application This page is located at https://github.com/settings/applications/new.\nYou can also reach it through GitHub‚Äôs menus:\nClick your profile picture and choose ‚ÄúSettings.‚Äù\nClick ‚ÄúDeveloper settings,‚Äù then make sure you‚Äôre on the ‚ÄúOAuth Apps‚Äù page (not ‚ÄúGitHub Apps‚Äù).\nClick the ‚ÄúNew OAuth App‚Äù button.\nFill it with the details you got above from the setup provider section and click on register application.\nNext click on generate a new client secret.\nNow go back to the Terraform Cloud, Enter the Client ID and Client Secret from the previous step, as well as an optional Name for this VCS connection. Click ‚ÄúConnect and continue.‚Äù This takes you to a page on GitHub.com, asking whether you want to authorize the app.\nClick the green ‚ÄúAuthorize \u0026quot; button at the bottom of the authorization page. GitHub might request your password to confirm the operation.\nNext, if the organization repositories include Git submodules that can only be accessed via SSH, an SSH key can be added along with the OAuth credentials. We will be skipping this step\nAnd that‚Äôs it, we have created a new VCS provider to connect to GitHub\nTerraform Cloud Workflows Terraform Cloud has three workflows for managing Terraform runs.\n**The UI/VCS-driven run workflow **‚Äî here you are connecting your VCS to Terraform Cloud ‚Äî easily integrate version control such as GitHub, GitLab, BitBucket or Azure DevOps and automatically initiate Terraform runs when changes are committed to the specified branch\n**The API-driven run workflow **‚Äî you can use your standard Terraform CLI to trigger remote runs.\n**The CLI-driven run workflow **‚Äî where you can manage and trigger runs through other tools by triggering calls to Terraform Cloud.\nHere in this blog, we will be more focusing on the VCS-driven workflow.\nVCS Driven Workflow Some of the features:\ninitiating speculative plans every time a PR is created against the default branch (this is set up by default so you don‚Äôt have to do anything)\nonce PR is merged this will trigger the plan and apply, however by default applying will require manual approval.\nyou have various triggers to choose from. I will discuss this more when coming to an example.\n*Sample repo containing some terraform code for testing can be found under: *https://github.com/NITHIN-JOHN-GEORGE/terraform-cloud-demo\nCreating a Workspace Click Projects \u0026amp; workspaces ‚Üí Click New, then select Workspace ‚Üí Choose a workflow type: Version control ‚Üí Choose a repository\nEnter a Workspace Name ‚Üí choose a Project to add the workspace to ‚Üí Open Advanced options to configure Terraform Working Directory, Set up AutoRun Triggering, and setting up the VCS branch.\nNote :\nTerraform Cloud provides different types of triggers to start a run automatically based on specific events. These triggers can be customized to suit the needs of your workflow.\nThe first type of trigger is the path changes trigger, which is particularly useful for monorepos where multiple projects are stored in a single repository. With this trigger, you can configure Terraform Cloud to start a run whenever a change occurs in a specific directory or file path within the repository. This means you don‚Äôt have to run the entire codebase and can focus on the relevant changes.\nThe second and recommended type of trigger is a pattern-based trigger. This trigger uses glob patterns to select which changes should trigger a run and ignore others. For example, if you only require a run when there is a change in .tf files in the ‚Äútfe-test‚Äù directory, you can specify the pattern as ‚Äú/tfe-test/*.tf‚Äù. This ensures that Terraform Cloud only runs when specific changes occur, reducing the risk of unintended consequences and speeding up the deployment process.\nThis is an important configuration to be noted, otherwise, the workspace will be triggered every time a code changes in any other sub-directory in the Github-Repo.\nNext Click on Create Workspace\nHere we can directly configure variables, or we can skip and do it afterward.\nNext, Go to Workspace Overview ‚Üí Click on Configure Variables\nHere you can add new variables which are required !!\nHere note we are adding a new variable called AWS_ACCESS_KEY. This is a variable that must be kept confidential so we have to click on Sensitive.\nHere we can also directly declare variables as Environment variables .\nRepeat the same for all variables you have.\nAs you can see the sensitive variables cannot be seen from the console, we also cannot see if we edit it, we can only update the value.\nThat‚Äôs it.\nNow go to Actions ‚Üí Start a new run\nIt will wait for our Confirmation for Applying. Click on Confirm \u0026amp; Apply\nWe can see the states got saved here.\nAnd we can see a list of all Runs that happened:\nNow, let\u0026rsquo;s test the Auto triggering Functionality !!\nLet\u0026rsquo;s modify some parts of code and as soon as you push the code to GitHub, the Terraform runs will be triggered\nAnd that\u0026rsquo;s it, it got triggered !!\nThis confirms Everything Working Fine as Expected !!\nThere are other features of TF cloud like private module registry, configuring TF-agents, sentinel policies, drift detection,cost-estimation, etc .. we will be covering some of them in the upcoming blogs.\nConfiguring TF agents in our environment where can have complete control of the infrastructure, sentinel policies, drift detection, and cost estimation is paid feature and comes under the Business Plan of Terraform cloud\nRecommendations while using TF cloud in production : Add notifications for all events / certain events to a slack channel for workspaces with the prod prefix\nUse self-hosted TF-agents\nUse TF cloud for hosting our private modules\nAlternatively, write a backup script that automatically sends the state to s3 and does cross-region replication for disaster-recovery purposes and create workspaces with the states from S3.\nProvide granular access to terraform workspaces across team members with access controls.\nSet the Terraform log level to debug\nUse Features like Drift Detection ( which continuously checks against the infrastructure state to detect and notify when there are changes), Cost Estimation, and Sentinel Policies ( granular policies about how the infrastructure should get provisioned ).\nThanks for Reading !! Happy Learning\n","permalink":"https://nithin-john-george.github.io/blog/terraform-cloud/","summary":"üîó Medium In this blog, we will discover how Terraform Cloud can help you streamline your cloud infrastructure management process and we will explore the powerful features that make it a must-have tool for cloud infrastructure management.\nWe will deliver the reasons why Terraform Cloud is essential, highlighting its capabilities and benefits, advanced workflows, and much more. We‚Äôll take a closer look at the VCS-driven workflow, providing a detailed example to illustrate how it works in practice.","title":"Automating Cloud Infrastructure with Terraform Cloud: A Deep Dive into its Capabilities"},{"content":"üîó Medium The Shell acts as an interface between the user and the kernel in an operating system. It allows users to interact with the system by providing a command-line or graphical user interface (GUI) through which they can execute commands or run applications. When a user enters a command, the Shell interprets it and communicates with the kernel to carry out the requested operation.\nAnd , a command is instruction that the user gives to the Shell to perform a specific task.\nSystem call A system call is a request made by a process running in user space to the operating system kernel to perform an operating system service on behalf of the process. When a program running in user space needs to perform an operating system service, it makes a system call by invoking a specific function provided by the operating system.\nSimply put , System calls are calls to the Kernel code to do something.\nHow does ls -l command works internally? When you enter a command, the first thing the shell does is it breaks the entire command into ‚Äútokens.‚Äù The shell will then look for a program name belonging to the first token in the command line.\nIn this example, ‚Äòls -l‚Äô is composed of two tokens, ‚Äòls‚Äô, and ‚Äò-l.‚Äô Next it checks for shell expansion i.e.,\nThe shell first verifies whether the first token entered by the user is an alias. If an alias exists, it replaces it with the corresponding actual command. Additionally, the shell expands any other aliases defined by the user before proceeding.\nIf there is no alias present , the shell first checks if the entered command is a built-in command or not. Then , it looks for the location of the executable program file, such as ‚Äúls,‚Äù in the system‚Äôs directories specified in the $PATH environment variable. The $PATH variable is a list of directories that the shell searches through whenever a command is entered.\nAfter identifying the $PATH variable, the shell breaks down all the directories in the $PATH using ‚Äò:‚Äô as a delimiter(tokenizing it) . If the shell fails to find the executable file in any of the directories listed in the $PATH variable, or if the command is neither a shell function nor an alias, the shell will throw an error.\nIf the command is located successfully, the shell examines the remaining tokens to identify if they are variables, shell parameters, or arguments to the command. In case they are variables or parameters, the shell will perform an expansion of these elements, replacing them with their original values in the command.\n*In Linux , most of the executable files are located in **‚Äò/usr/bin‚Äô . *The binary executable file of ls also will be there ( ‚Äò/usr/bin/ls‚Äô )\nAfter finding the ‚Äúls‚Äù executable file, the shell creates a new process, which is an instance of a program that needs to be executed. To execute the ‚Äúls‚Äù command, the shell uses three system calls: fork, , and wait. The shell duplicates itself (making a copy of itself) using the fork() system call, creating a child process that is a copy of the parent process (the shell).\nAfter creating the child process with the fork system call, the child process runs the execve system call ( execution system call ) with the path to the ‚Äúls‚Äù executable and its arguments.\nThen the system program will use some system calls to load all the libraries required by the program into the virtual memory at run-time ( dynamic loading )\nWe can use strace command to trace all system calls happening.\nWhile the child process executes it, the parent process waits for execution to be completed and once the execution call ends, the child process will indicate that it has finished and returns the response to the parent process through another system call called wait() and then the child process is terminated with the exit() system .\nOnce this execution process has been carried out, the memory used in these processes is freed, the response or result of the command will be displayed on the user‚Äôs screen, and the parent process takes over again and waits for the next input from the user.\nIn a program where you use fork, you also have to use wait() system call. wait() system call is used to wait in the parent process for the child process to finish.\nThank you for reading my article , have a nice day and keep learning!\n","permalink":"https://nithin-john-george.github.io/blog/internals-of-ls-a-command/","summary":"üîó Medium The Shell acts as an interface between the user and the kernel in an operating system. It allows users to interact with the system by providing a command-line or graphical user interface (GUI) through which they can execute commands or run applications. When a user enters a command, the Shell interprets it and communicates with the kernel to carry out the requested operation.\nAnd , a command is instruction that the user gives to the Shell to perform a specific task.","title":"What happens when you type ls -a in the shell"},{"content":"üîó GitHub Overview This Terraform configuration is designed for creating and managing a high-availability (HA) Virtual Private Cloud (VPC) in AWS. It\u0026rsquo;s tailored to support multiple environments (prod, qa, dev) . It includes a dynamic count mechanism to control resource creation based on the environment (prod, qa, dev)\nmain.tf main.tf is the central file that defines the resources to be created and managed in AWS.\nAWS Resources Created AWS VPC (aws_vpc)\nA Virtual Private Cloud (VPC) is created with configurable properties like CIDR block, DNS support, and instance tenancy. Subnets (aws_subnet)\nBoth public and private subnets are defined, providing network segments for different use cases within the VPC. The creation of subnets is dynamic, based on the count of CIDR blocks specified for public and private subnets variables. This is achieved through the count attribute in Terraform and the variables public_subnets_cidr and private_subnets_cidr. Example: If public_subnets_cidr is [\u0026quot;10.0.1.0/24\u0026quot;, \u0026quot;10.0.2.0/24\u0026quot;], two public subnets will be created with these CIDR blocks. Internet Gateway (aws_internet_gateway)\nAn Internet Gateway is attached to the VPC, enabling communication between resources in the VPC and the internet. Route Tables (aws_route_table)\nRoute tables for public and private (including production) networking are established, defining rules for network traffic routing. The public route table usually contains a default route that directs traffic to an Internet Gateway (IGW), enabling instances in the public subnet to access the internet. Private route tables are typically associated with NAT gateways, allowing instances in private subnets to access the internet indirectly while keeping them inaccessible from the internet. Routes (aws_route)\nSpecific routes are defined, like the default public internet gateway route and routes for NAT gateways, guiding the traffic flow. Route Table Associations (aws_route_table_association)\nThese associations connect subnets to the appropriate route tables. Each subnet, whether public or private, needs to be associated with a route table. Public subnets are typically associated with a route table that has a route to an Internet Gateway. Private subnets are associated with route tables that have routes to NAT gateways, enabling them to access the internet without being directly exposed to it. Elastic IPs (aws_eip)\nThey are used in conjunction with NAT gateways in AWS to provide a fixed, public IP address for instances within a private subnet to communicate with the internet. In environments like development or QA, a single EIP may be sufficient. In a production environment, where high availability and fault tolerance are critical, an EIP is typically created for each NAT gateway. Since NAT gateways are created per availability zone for redundancy, this would mean one EIP per AZ in a production setup. NAT Gateways (aws_nat_gateway)\nNAT gateways are created for enabling outbound internet access for instances in the private subnets. In non-production environments, a single NAT gateway is typically sufficient. The configuration uses a conditional check to determine if the environment is production. If not, a single NAT gateway is created. For production environments, the configuration is likely set to create a NAT gateway in each availability zone for high availability. The Terraform script checks the length of the list of availability zones (AZs) and creates a NAT gateway for each AZ. Flow Logs (aws_flow_log)\nThe creation of flow logs in your Terraform script is conditional The destination for these flow logs can be either an Amazon S3 bucket or a CloudWatch Logs log group. This is determined by the value of the vpc_flowlog_destination_type variable in your Terraform configuration S3 Bucket (aws_s3_bucket)\nAn S3 bucket is created for storing VPC flow logs. S3 Bucket ACL and Versioning (aws_s3_bucket_acl, aws_s3_bucket_versioning) Access control lists and versioning settings are defined for the S3 bucket. S3 Bucket Lifecycle Configuration (aws_s3_bucket_lifecycle_configuration) This resource manages the lifecycle of objects within the S3 bucket. CloudWatch Log Group (aws_cloudwatch_log_group) A log group in CloudWatch is created for the VPC flow logs. IAM Role and Policy (aws_iam_role, aws_iam_role_policy) An IAM role and corresponding policy are established for managing permissions related to the flow logs. Network ACL (aws_network_acl) A network access control list (ACL) is created, providing an additional layer of security for the VPC subnets. Name Type aws_cloudwatch_log_group.vpc_flowlog_loggroup resource aws_eip.nat_eip resource aws_eip.nat_eip_prod resource aws_flow_log.vpc_flowlog resource aws_flow_log.vpc_flowlog_Cloudwatch resource aws_iam_role.vpcflow_log_group_role resource aws_iam_role_policy.vpc_flowloggroup_role_policy resource aws_internet_gateway.ig resource aws_nat_gateway.nat resource aws_nat_gateway.prodnat resource aws_network_acl.default_network_acl resource aws_route.default_prod_route_nat_gateway resource aws_route.default_public_internet_gateway_route resource aws_route.default_route_nat_gateway resource aws_route_table.private resource aws_route_table.prod_private resource aws_route_table.public resource aws_route_table_association.private resource aws_route_table_association.prod_private resource aws_route_table_association.public resource aws_s3_bucket.vpc_flowlog_bucket resource aws_s3_bucket_acl.vpc_flowlog_bucket_acl resource aws_s3_bucket_lifecycle_configuration.vpc_flowlog_bucket_lifecycle resource aws_s3_bucket_versioning.vpc_flowlog_bucket_versioning resource aws_subnet.private resource aws_subnet.public resource aws_vpc.vpc resource aws_caller_identity.current data source output.tf Defines output variables that provide essential information about the resources created by the Terraform configuration.\nOutputs\nName Description account_id Account-ID natip NAT IP private_subnet_id Identifiers for the private subnets created within the VPC prodnatip Prod NAT IP public_subnet_id Identifiers for the public subnets created within the VPC vpc_arn VPC ARN vpc_id VPC ID variables.tf Declares and describes the input variables used in the configuration.\nName Description Type Default Required AWS_ACCESS_KEY to run terraform code in workspace please enter your AWS_ACCESS_KEY any n/a yes AWS_SECRET_KEY to run terraform code in workspace please enter your AWS_SECRET_KEY any n/a yes availability_zones please enter your availability_zones for your subnets in a list of string note =\u0026gt; if you want 3 subnets in 3 differnt zone plaese enter 3 zones. You can follow single avilablility zone or multi availability zone method as your wish list(string) n/a yes eks_cluster_name Assume your eks cluster name and enter here, this is used under your vpc tags string n/a yes env please enter your one environment name eg dev,qa or prod string n/a yes private_route_table_routes if you have a route, please eneter your private routes as key value pairs eg [{},{}] otherwise leave it\u0026hellip;.. in a pair keys are ipv6_cidr_block,egress_only_gateway_id,gateway_id,instance_id,nat_gateway_id,network_interface_id,transit_gateway_id,vpc_endpoint_id,vpc_peering_connection_id eg =\u0026gt; [{ cidr_block = ,ipv6_cidr_block = ,egress_only_gateway_id = ,gateway_id = ,instance_id = ,nat_gateway_id = ,network_interface_id = ,transit_gateway_id = ,vpc_endpoint_id = ,vpc_peering_connection_id = },{}\u0026hellip; ] list(map(string)) [] no private_subnet_tags please enter a custom tag for your private_subnet in a key value pair map(string) n/a yes private_subnets_cidr please enter your private subnet\u0026rsquo;s cidr\u0026rsquo;s in a list of string note =\u0026gt; if you want 3 private subnets you can enter 3 cidrs ranges for that list(string) n/a yes public_route_table_routes if you have a route, please eneter your private routes as key value pairs eg [{},{}] otherwise leave it\u0026hellip;.. in a pair keys are ipv6_cidr_block,egress_only_gateway_id,gateway_id,instance_id,nat_gateway_id,network_interface_id,transit_gateway_id,vpc_endpoint_id,vpc_peering_connection_id eg =\u0026gt; [{ cidr_block = ,ipv6_cidr_block = ,egress_only_gateway_id = ,gateway_id = ,instance_id = ,nat_gateway_id = ,network_interface_id = ,transit_gateway_id = ,vpc_endpoint_id = ,vpc_peering_connection_id = },{}\u0026hellip; ] list(map(string)) [] no public_subnet_tags please enter a custom tag for your public_subnet_tags in a key value pair map(string) n/a yes public_subnets_cidr please enter your public subnet\u0026rsquo;s cidr\u0026rsquo;s in a list of string. note =\u0026gt; if you want 3 public subnets you can enter 3 cidrs ranges for that list(string) n/a yes region please enter a valid region for vpc launching string n/a yes vpc_cidr please enter a valid cidr range for your vpc in string eg: 10.1.0.0/16 string n/a yes vpc_flowlog please enter true or false bool n/a yes vpc_flowlog_bucket_retention please eneter your flowlog s3 bucket retention period in number of days number n/a yes vpc_flowlog_destination_type please enter your vpc flowlog backend type s3 or cloudwatch_log_group string n/a yes vpc_name please enter a vpc_name for your new vpc string n/a yes vpc_tags please enter a custom tag for your vpc in a key value pair map(string) n/a yes versions.tf Specifies the required Terraform version and AWS provider version.\nRequirements\nName Version terraform ~\u0026gt; 1.0 aws ~\u0026gt; 4.54.0 Providers\nName Version aws ~\u0026gt; 4.54.0 Resource Creation in TF cloud We Will be Creating a VPC with 3 public and 3 private subnets with Terraform and managed via terraform cloud for managing remote state , workspace variables , plans and apply\n","permalink":"https://nithin-john-george.github.io/projects/terraform-vpc/","summary":"üîó GitHub Overview This Terraform configuration is designed for creating and managing a high-availability (HA) Virtual Private Cloud (VPC) in AWS. It\u0026rsquo;s tailored to support multiple environments (prod, qa, dev) . It includes a dynamic count mechanism to control resource creation based on the environment (prod, qa, dev)\nmain.tf main.tf is the central file that defines the resources to be created and managed in AWS.\nAWS Resources Created AWS VPC (aws_vpc)","title":"Production-Grade VPC with Environment-Specific Terraform Configuration"},{"content":"üîó GitHub üîó Medium Apache Log Parser With Python As part of this DevOps project, we showcase how Python scripting can be used to parse Apache access logs in real-world scenarios. Two years ago, I posted a shell script (https://github.com/NITHIN-JOHN-GEORGE/apache_log_parser) for performing this operation. The following is a Python implementation of the same.\nConcepts in Python used in this Project: Regex with re module\nArg parser Module\nFunctions\nOS module\nsys module\ncsv and datetime module\nCounter from Collection Module\nLoops\nConditional Statements\nAbout The Project The script allows users to analyze and extract specific data from Apache access log files. The Apache access log file is a text file that contains information about requests made to the Apache webserver. This information includes details such as the IP address of the client making the request, the request method used, the time and date of the request, the requested URL, and the HTTP status code returned by the server.\nThe script accepts user input parameters, including the location of the Apache access log file to parse, the type of operation to perform, and the output format of the result (either table or CSV). It uses the argparse module to parse user input parameters.\nThe script is designed to accept three user input parameters:\nthe location of the Apache access log file to parse\nthe type of operation to perform.\nand the output format of the result (either table or CSV)\nThe script can perform four types of operations.\nFirst, it can count the most frequently visited IP addresses, returning the IP addresses with the highest number of hits. The script uses regular expressions to extract IP addresses from the log file and the Counter module to count the number of occurrences of each IP address.\nSecond, it can count the number of times each request method (GET, POST, PUT, or DELETE) appears in the log file and returns the methods with the highest number of hits.\nThird, it can count the number of times each base URL (the part of the URL before the query string) appears in the log file and returns the URLs with the highest number of hits. The script uses regular expressions to extract base URLs from the log file and the Counter module to count the number of occurrences of each base URL.\nFinally, it can count the number of times each HTTP status code appears in the log file and returns the codes with the highest number of hits.\nThe script uses regular expressions to extract the relevant data from the log file and the Counter module to count the number of occurrences of each data point. It also includes helper functions for checking if the log file exists and for printing headers and centering text .The script uses the os and datetime modules to generate timestamps and get the size of the terminal window.\nOnce the script has performed the requested operation, it outputs the results either to the console or a CSV file, depending on the user‚Äôs specified output format. If the output format is ‚Äútable,‚Äù the script formats the output as a table using the print function. If the output format is ‚ÄúCSV,‚Äù the script writes the output to a CSV file using the csv module.\nProject Repository : https://github.com/NITHIN-JOHN-GEORGE/apache-log-parser-python\nA sample dataset for Apache access.log can be found here ( only for demo) :\nRunning the script:\n- git clone \u0026lt;https://github.com/NITHIN-JOHN-GEORGE/apache-log-parser-python.git\u0026gt; - cd Apache-log-parser-python - python3 apache-log-parser.py -h Running the Help of the script:\nRunning the script without any argument:\nCount the most frequently visited IP addresses: **python3 apache-log-parser.py -l access.log -o most-visited-ips -f table ** # Note : To generate a csv report you can use **python3 apache-log-parser.py -l access.log -o most-visited-ips -f csv ** # It will generate csv file with current timestamp of file creation # Example: most_visited_ip_2023-04-24-23-41-38.csv Counting the number of times each request method in the log file: **python3 apache-log-parser.py -l access.log -o top-request-methods -f table ** # Note : To generate a csv report you can use **python3 apache-log-parser.py -l access.log -o most-visited-ips -f csv ** # It will generate csv file with current timestamp of file creation # Example: top_request_methods_2023-04-24-23-48-05.csv Counting the number of times each base URL appears in the log file **python3 apache-log-parser.py -l access.log -o top-base-urls -f table ** # Note : To generate a csv report you can use **python3 apache-log-parser.py -l access.log -o top-base-urls -f csv ** # It will generate csv file with current timestamp of file creation # Example: top_base_urls_2023-04-24-23-52-52.csv Counting the number of times each HTTP status code appears in the log file **python3 apache-log-parser.py -l access.log -o top-http-status-codes -f table ** # Note : To generate a csv report you can use **python3 apache-log-parser.py -l access.log -o top-http-status-codes -f csv ** # It will generate csv file with current timestamp of file creation # Example: top_base_urls_2023-04-24-23-52-52.csv Thats all ! Thank you for reading my article , have a nice day and keep learning!\n","permalink":"https://nithin-john-george.github.io/projects/apache-log-parser-python/","summary":"üîó GitHub üîó Medium Apache Log Parser With Python As part of this DevOps project, we showcase how Python scripting can be used to parse Apache access logs in real-world scenarios. Two years ago, I posted a shell script (https://github.com/NITHIN-JOHN-GEORGE/apache_log_parser) for performing this operation. The following is a Python implementation of the same.\nConcepts in Python used in this Project: Regex with re module\nArg parser Module\nFunctions\nOS module","title":"Apache Web Server Log Parsing Tool Using Python (Enhanced Version)"},{"content":"Description Project on End to End CI/CD pipeline for java based application using Git,Github,Jenkins,Maven,Sonarqube,Nexus,Slack,Docker and Kuberenets with ECR as private docker registry and Zero Downtime Deployment.\nProject Flow 1. Developer pushes code into Github. 2. Webhook triggers Jenkins when there is a change in the code 3. Jenkins Pulls the Code from the Github 4. Maven builds the code and generates artifacts 5. Code quality is measured with Sonarqube 6. Quality Gate Check , If Quality Gate Fails Jenkins Job will fail !!!!!! (Triggered by Sonarqube Webhooks) 7. Upload artifact generated into Sonatype Nexus . It will dynamically choose Snapshot or release repository based on the version tag in pom.xml 8. Build Docker Image based on the Dockerfile with projectname \u0026amp;\u0026amp; commit-id as tag . So each time it will be different. 9. Push Docker Image to private ECR docker registry. 10.Dynamically change image in pod template in manifest file. 11.Deploy to K8s cluster created with kubeadm . It will pull image from private registry. 12. Send Build Notification over Slack Channel and email notification when build is success/failure. Note: We can add an approval step before deploying to K8s cluster as an input from user. Features 1. Zero downtime deployment with rolling update as deployment strategy 2. Complete automation as when developer check in code , deployed to k8s cluster 3. Versioning of docker images , build artifacts. 4. Code checked against Code coverage and whether coding stantards are met. Pipeline Execution Execution Results SONARQUBE REPORTS\nNEXUS UPLOADING\nECR\nJOB TRIGGERED BY WEBHOOKS\nK8s CLUSTER\nController --\u0026gt; Deployment Strategy --\u0026gt; Rolling Update SLACK NOTIFICATION\nEMAIL NOTIFICATION\nFINAL RESULT\nJenkins Pipeline Change account number and credentials as per your environment.\ndef COMMIT def BRANCH_NAME def GIT_BRANCH pipeline { agent any environment { AWS_ACCOUNT_ID=\u0026#34;\u0026lt;Your account Id\u0026gt;\u0026#34; AWS_DEFAULT_REGION=\u0026#34;us-east-1\u0026#34; IMAGE_REPO_NAME=\u0026#34;mavenwebapp\u0026#34; REPOSITORY_URI = \u0026#34;${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${IMAGE_REPO_NAME}\u0026#34; } tools { maven \u0026#39;MAVEN_3.8.4\u0026#39; } options { buildDiscarder logRotator(artifactDaysToKeepStr: \u0026#39;\u0026#39;, artifactNumToKeepStr: \u0026#39;4\u0026#39;, daysToKeepStr: \u0026#39;\u0026#39;, numToKeepStr: \u0026#39;4\u0026#39;) timestamps() } stages { stage(\u0026#39;Code checkout\u0026#39;) { steps { script { checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/development\u0026#39;]], extensions: [], userRemoteConfigs: [[url: \u0026#39;https://github.com/NITHIN-JOHN-GEORGE/JAVA-CI-CD-PIPELINE.git\u0026#39;]]]) COMMIT = sh (script: \u0026#34;git rev-parse --short=10 HEAD\u0026#34;, returnStdout: true).trim() } } } stage(\u0026#39;Build\u0026#39;) { steps { sh \u0026#34;mvn clean package\u0026#34; } } stage(\u0026#39;Execute Sonarqube Report\u0026#39;) { steps { withSonarQubeEnv(\u0026#39;Sonarqube-Server\u0026#39;) { sh \u0026#34;mvn sonar:sonar\u0026#34; } } } stage(\u0026#39;Quality Gate Check\u0026#39;) { steps { timeout(time: 1, unit: \u0026#39;HOURS\u0026#39;) { waitForQualityGate abortPipeline: true, credentialsId: \u0026#39;SONARQUBE-CRED\u0026#39; } } } stage(\u0026#39;Nexus Upload\u0026#39;) { steps { script { def readPom = readMavenPom file: \u0026#39;pom.xml\u0026#39; def nexusrepo = readPom.version.endsWith(\u0026#34;SNAPSHOT\u0026#34;) ? \u0026#34;wallmart-snapshot\u0026#34; : \u0026#34;wallmart-release\u0026#34; nexusArtifactUploader artifacts: [ [ artifactId: \u0026#34;${readPom.artifactId}\u0026#34;, classifier: \u0026#39;\u0026#39;, file: \u0026#34;target/${readPom.artifactId}-${readPom.version}.war\u0026#34;, type: \u0026#39;war\u0026#39; ] ], credentialsId: \u0026#39;Nexus-Cred\u0026#39;, groupId: \u0026#34;${readPom.groupId}\u0026#34;, nexusUrl: \u0026#39;3.82.213.203:8081\u0026#39;, nexusVersion: \u0026#39;nexus3\u0026#39;, protocol: \u0026#39;http\u0026#39;, repository: \u0026#34;${nexusrepo}\u0026#34;, version: \u0026#34;${readPom.version}\u0026#34; } } } stage(\u0026#39;Login to AWS ECR\u0026#39;) { steps { script { sh \u0026#34;/usr/local/bin/aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\u0026#34; } } } stage(\u0026#39;Building Docker Image\u0026#39;) { steps { script { sh \u0026#34;docker build . -t ${REPOSITORY_URI}:mavenwebapp-${COMMIT}\u0026#34; } } } stage(\u0026#39;Pushing Docker image into ECR\u0026#39;) { steps { script { sh \u0026#34;docker push ${REPOSITORY_URI}:mavenwebapp-${COMMIT}\u0026#34; } } } stage(\u0026#39;Update image in K8s manifest file\u0026#39;) { steps { sh \u0026#34;\u0026#34;\u0026#34;#!/bin/bash sed -i \u0026#39;s/VERSION/$COMMIT/g\u0026#39; deployment.yaml \u0026#34;\u0026#34;\u0026#34; } } stage(\u0026#39;Deploy to K8s cluster\u0026#39;) { steps { sh \u0026#39;kubectl apply -f deployment.yaml --record=true\u0026#39; sh \u0026#34;\u0026#34;\u0026#34;#!/bin/bash sed -i \u0026#39;s/$COMMIT/VERSION/g\u0026#39; deployment.yaml \u0026#34;\u0026#34;\u0026#34; } } } post { always { cleanWs() } success { slackSend channel: \u0026#39;build-notifications\u0026#39;,color: \u0026#39;good\u0026#39;, message: \u0026#34;started JOB : ${env.JOB_NAME} with BUILD NUMBER : ${env.BUILD_NUMBER} BUILD_STATUS: - ${currentBuild.currentResult} To view the dashboard (\u0026lt;${env.BUILD_URL}|Open\u0026gt;)\u0026#34; emailext attachLog: true, body: \u0026#39;\u0026#39;\u0026#39;BUILD IS SUCCESSFULL - $PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS: Check console output at $BUILD_URL to view the results. Regards, Nithin John George \u0026#39;\u0026#39;\u0026#39;, compressLog: true, replyTo: \u0026#39;\u0026lt;Your mail id\u0026gt;\u0026#39;, subject: \u0026#39;$PROJECT_NAME - $BUILD_NUMBER - $BUILD_STATUS\u0026#39;, to: \u0026#39;njdevops321@gmail.com\u0026#39; } failure { slackSend channel: \u0026#39;build-notifications\u0026#39;,color: \u0026#39;danger\u0026#39;, message: \u0026#34;started JOB : ${env.JOB_NAME} with BUILD NUMBER : ${env.BUILD_NUMBER} BUILD_STATUS: - ${currentBuild.currentResult} To view the dashboard (\u0026lt;${env.BUILD_URL}|Open\u0026gt;)\u0026#34; emailext attachLog: true, body: \u0026#39;\u0026#39;\u0026#39;BUILD IS FAILED - $PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS: Check console output at $BUILD_URL to view the results. Regards, Nithin John George \u0026#39;\u0026#39;\u0026#39;, compressLog: true, replyTo: \u0026#39;\u0026lt;Your mail id\u0026gt;\u0026#39;, subject: \u0026#39;$PROJECT_NAME - $BUILD_NUMBER - $BUILD_STATUS\u0026#39;, to: \u0026#39;\u0026lt;Your mail id\u0026gt;\u0026#39; } } } K8s manifest File (Deployment) apiVersion: apps/v1 kind: Deployment metadata: name: mavenwebapp-dp labels: app: mavenwebapp spec: replicas: 4 revisionHistoryLimit: 10 strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 1 minReadySeconds: 30 selector: matchLabels: app: mavenwebapp template: metadata: name: mavenwebapp-pod labels: app: mavenwebapp spec: imagePullSecrets: - name: regcrd containers: - name: mavenwebapp-container image: \u0026lt;AWS_ACCOUNT_NUMBER\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/mavenwebapp:mavenwebapp-VERSION imagePullPolicy: Always ports: - containerPort: 8080 resources: requests: cpu: 300m memory: 256Mi limits: cpu: 800m memory: 1Gi --- apiVersion: v1 kind: Service metadata: name: mavenwebapp-svc spec: type: NodePort selector: app: mavenwebapp ports: - port: 8080 targetPort: 8080 nodePort: 30003 Docker File FROM tomcat:8.0.20-jre8 COPY target/java-web-app*.war /usr/local/tomcat/webapps/java-web-app.war ","permalink":"https://nithin-john-george.github.io/projects/end-to-end-ci-cd-pipeline/","summary":"Description Project on End to End CI/CD pipeline for java based application using Git,Github,Jenkins,Maven,Sonarqube,Nexus,Slack,Docker and Kuberenets with ECR as private docker registry and Zero Downtime Deployment.\nProject Flow 1. Developer pushes code into Github. 2. Webhook triggers Jenkins when there is a change in the code 3. Jenkins Pulls the Code from the Github 4. Maven builds the code and generates artifacts 5. Code quality is measured with Sonarqube 6. Quality Gate Check , If Quality Gate Fails Jenkins Job will fail !","title":"End to End CI/CD pipeline for java based application on K8s"},{"content":"üîó GitHub Description It\u0026rsquo;s a bash script to parse an apache access log file.\nAnalysizing Apache log files to count most visited Unique Ip , to see http status codes , request method and most viewed unique URLS manually is an not an easy task due to the size of log file and less readability of log file.\nEvery time a web page is requested, an email sent or a login made, the event is recorded in a log file. An essential skill for any system administrator is to be to able parse through log files and extract useful information.\nThis script can be used for this purpose:\nFeature The script will extract data from log file such as :\nIDENTIFY FROM WHICH IPS MY SERVER HAS BEEN ACCESSED MORE AND HOW MANY TIMES (TOP 20 IP ADDRESSES ) View request types GET/HEAD/POST/DELETE that are happening the most View highest 20 requested unique base URLs. View the most common response codes your visitors are causing. How to use this script - To get sample access log for practice purpose use: wget http://www.almhuette-raith.at/apache-log/access.log - git clone https://github.com/NITHIN-JOHN-GEORGE/apache_log_parser.git - cd apache_log_parser - In prod environment where real access log is available , update the access log file path in script starting Log_File=\u0026lt;apache access log file absolute path\u0026gt; - ./apache_log_analyser.sh Script Running ","permalink":"https://nithin-john-george.github.io/projects/apache-log-parser-shell-script/","summary":"üîó GitHub Description It\u0026rsquo;s a bash script to parse an apache access log file.\nAnalysizing Apache log files to count most visited Unique Ip , to see http status codes , request method and most viewed unique URLS manually is an not an easy task due to the size of log file and less readability of log file.\nEvery time a web page is requested, an email sent or a login made, the event is recorded in a log file.","title":"Apache Web Server Log Parsing Tool Using Shell Scripting"},{"content":"üîó GitHub Description Its a shell script which can be used to monitor CPU , Memory , Disk Space of an Linux Server based on the threshold we pass and send email alerts when the usage is above the given threshold . This script also generates list of top processes which takes high cpu and high memory.\nMonitoring servers resources is an integral part of every organization . For this we can use Monitoring tools like nagios , zabbix etc .. but here i made use of shell script as a part of learning shell script and can be set as a cronjob so it will periodically check the usage of server resources. This script can be helpful for system engineers/devops engineers for monitoring their server resources.\nFeature The script will monitor filesystem (disk usage) , memory utilization and CPU utilization of Linux Server We can set threshold limit (critical \u0026amp; warning limits) If the usage goes above mentioned threshold , it will automatically send an email to administrator saying the resource usage is in CRITICAL or WARNING state. For CPU and Memory It gives top processes which takes more CPU and memory and send that along with mail. Pre-Requisite üîó GitHub If you want to send mail when usage becomes high a mail agent should be installed on the server.Here we can use sendmail. STEPS TO CONFIGURE GMAIL SETUP ON UBUNTU SERVER step1: Get Gmail Id and Password Step2: login into ubuntu and switch to root using: sudo su - Step3: Run below commands: apt-get update -y apt-get install sendmail mailutils -y Step4: Create authentication file cd /etc/mail mkdir -m 700 authinfo cd authinfo/ vi gmail add the below conntent AuthInfo: \u0026#34;U:root\u0026#34; \u0026#34;I:your-mail@gmail.com\u0026#34; \u0026#34;P:your-password\u0026#34; Now edit your mail id and password Step5: create hash map of the file: makemap hash gmail \u0026lt; gmail Step6: Got to /etc/mail and open sendmail.mc then Add the following lines to sendmail.mc file right above MAILER_DEFINITIONS: #GMail settings: define(`SMART_HOST\u0026#39;,`[smtp.gmail.com]\u0026#39;)dnl define(`RELAY_MAILER_ARGS\u0026#39;, `TCP $h 587\u0026#39;)dnl define(`ESMTP_MAILER_ARGS\u0026#39;, `TCP $h 587\u0026#39;)dnl define(`confAUTH_OPTIONS\u0026#39;, `A p\u0026#39;)dnl TRUST_AUTH_MECH(`EXTERNAL DIGEST-MD5 CRAM-MD5 LOGIN PLAIN\u0026#39;)dnl define(`confAUTH_MECHANISMS\u0026#39;, `EXTERNAL GSSAPI DIGEST-MD5 CRAM-MD5 LOGIN PLAIN\u0026#39;)dnl FEATURE(`authinfo\u0026#39;,`hash -o /etc/mail/authinfo/gmail.db\u0026#39;)dnl Step7: Now run below two command from /etc/mail make /etc/init.d/sendmail reload Step8: Now open https://www.google.com/settings/security/lesssecureapps and Allow less secure apps: ON Step9: Verify the test mail using echo \u0026#34;Demo\u0026#34; | mail -s \u0026#34;Status of Httpd\u0026#34; dowithscripting@gmail.com echo \u0026#34;Demo\u0026#34; | mail -s \u0026#34;Status of Httpd\u0026#34; dowithscripting@gmail.com -A demo.txt How to use this script - git clone https://github.com/NITHIN-JOHN-GEORGE/disk_cpu_mem_monitor.git - cd disk_cpu_mem_monitor - chmod +x resources_cpu_disk_mem.sh - OPEN THE SCRIPT AND CHANGE THE THRESHOLD VALUES ACCORDING TO OUR ORGANIZATION DISK_THRESHOLD=\u0026lt;\u0026gt; CPU_THRESHOLD_WARN=\u0026lt;\u0026gt; CPU_THRESHOD_CRITICAL=\u0026lt;\u0026gt; MEM_CRITICAL=\u0026lt;\u0026gt; MEM_WARNING=\u0026lt;\u0026gt; -Add a cronjob so it can monitor every 10 mins # crontab -e */10 * * * * \u0026lt;complete path to this script\u0026gt; Script Running ","permalink":"https://nithin-john-george.github.io/projects/disk_cpu_mem_monitor/","summary":"üîó GitHub Description Its a shell script which can be used to monitor CPU , Memory , Disk Space of an Linux Server based on the threshold we pass and send email alerts when the usage is above the given threshold . This script also generates list of top processes which takes high cpu and high memory.\nMonitoring servers resources is an integral part of every organization . For this we can use Monitoring tools like nagios , zabbix etc .","title":"Disk CPU Memory Monitor Using Shell Scripting"},{"content":"üîó GitHub Description It\u0026rsquo;s a bash script to show a complete inventory of an Linux OS and it might be useful for Linux system/support engineers who lot of servers in daily life . Instead of manually executing all the commands on each of server which is hell lot of task to do , you can execute this script and this script generates all needed inventory to a file which you can mail to your manager.\nThis is a small project done by me after studying shell scripting and i have used mix of different concepts like functions , conditional statements , loops , exit status , command chaining , and used different text processing commands like awk ,sed , grep , cut etc \u0026hellip;\nFeature The script will collect informations from OS such as :\nOS Details ( such as Hostname , OS_name , OS_version , OS_Kernel_Version ) Hypervisor Details CPU Details ( such as No of cores , CPU type , CPU usage , Load Average of server) Memory Details Disk Detais Network Detail ( like IP address , DNS SERVERS , DNS_DOMAIN , Total no of interfaces , List of interfaces ) SELINUX status SWAP details Pre-Requestes netstat package must be available Passwordless SSH authentication b/w servers ( Note If you are using password environment u have to use sshpass utility to provide password ) Common user accross all servers Sudo privilege to that user This script currently supports only rhel 7.x and 8.x (ubuntu servers will be added in future release)\nHow to use this script - Create a user common in all servers and make passwordless authentication connection b/w the servers. (Give sudo access to the user) - Switch to that user. - git clone https://github.com/NITHIN-JOHN-GEORGE/MULTIPLE-SERVER-INVENTORY-SHELL-SCRIPT.git - cd MULTIPLE-SERVER-INVENTORY-SHELL-SCRIPT - chmod +x MULTIPLE_SERVER_INVENTORY.sh - Create a servers_list.txt with updated list of server in which you need inventory. - Update the username in the script starting USER=\u0026lt;username\u0026gt; - ./MULTIPLE_SERVER_INVENTORY.sh Script Running ","permalink":"https://nithin-john-george.github.io/projects/server-inventory-shell-script/","summary":"üîó GitHub Description It\u0026rsquo;s a bash script to show a complete inventory of an Linux OS and it might be useful for Linux system/support engineers who lot of servers in daily life . Instead of manually executing all the commands on each of server which is hell lot of task to do , you can execute this script and this script generates all needed inventory to a file which you can mail to your manager.","title":"Generating Multiple Server Inventory Using Shell Scripting"},{"content":"Github Link üîó GitHub ","permalink":"https://nithin-john-george.github.io/projects/auto-heal-script/","summary":"Github Link üîó GitHub ","title":"Service Autohealer Using Shell Script"},{"content":"üîó GitHub ","permalink":"https://nithin-john-george.github.io/projects/install-and-configure-httpd-with-ansible/","summary":"üîó GitHub ","title":"Ansible Playbook to Install \u0026 Configure HTTPD Web Server with Jinja2 Templating"},{"content":" On-Demand Staging Box setup with Terraform , Python , ECS and Github-Actions Self Hosted Runners for Github Action on K8s with Karpenter Jenkins to Github-Actions Migration Docker Build Optimization AWS Cost-Tracker Automation with Python (Boto3) Cost Attribution ","permalink":"https://nithin-john-george.github.io/experience/porter/","summary":" On-Demand Staging Box setup with Terraform , Python , ECS and Github-Actions Self Hosted Runners for Github Action on K8s with Karpenter Jenkins to Github-Actions Migration Docker Build Optimization AWS Cost-Tracker Automation with Python (Boto3) Cost Attribution ","title":"Devops SDE-2 (Cloud Infrastructure)"},{"content":"Description Established efficient release engineering process incorporating git branching strategies for the organization. Managed Git Repositories, including branching and merging, and pull requests to manage codebase changes and collaborate with development teams. Responsible for setting up the fully automated CI/CD pipeline for Kubernetes Environment incorporating GitOps with Jenkins \u0026amp; ArgoCD Was responsible for setting up an automated Jenkins Pipeline for Lambda Deployment. Dockerized Node Js and Ruby on Rails Applications and pushed them to ECR utilizing efficient image tagging and optimized image size by using Multistage Dockerfile. Used Terraform (IAC) to create a wide range of scalable infrastructure on both AWS and GCP platforms, including networking components, EKS, GKE, RDS, Data pipeline infra, SSM, KMS, Lambda, and its associated triggers. Set UP Data pipeline infra to track data changes from MongoDB, MySQL, and Postgres using Terraform. Used Terraform Cloud as a CI/CD tool for cloud infrastructure automation. Migrated the applications running in the cloud vm to the Kubernetes cluster. Deployed multiple applications to Kubernetes, exposing them through the ALB Ingress controller to provide external access to services. Troubleshoot and resolve issues in Kubernetes clusters Access management to Developers to databases and AWS accounts utilizing IAM roles and policies to grant developers secure access to resources. Used Cloudflare for rate limiting, DDoS protection, setting WAF rules to restrict the domain to access over VPN, Pointing DNS records Implemented Cloudflare Health check and Cloudwatch Synthetic canaries to check the status of APIs and integrated with Opsgenie and Slack to get alerts Implemented Cloud Watch Monitoring for AWS resources and integrated with Opsgenie and Slack to get alerts based on Priority levels. Implemented Site to Site VPN connection between AWS and Perimeter 81 network for accessing the internal VPC network via VPN connections. Infrastructure Cost Optimization to reduce billings ","permalink":"https://nithin-john-george.github.io/experience/aptonworks/","summary":"Description Established efficient release engineering process incorporating git branching strategies for the organization. Managed Git Repositories, including branching and merging, and pull requests to manage codebase changes and collaborate with development teams. Responsible for setting up the fully automated CI/CD pipeline for Kubernetes Environment incorporating GitOps with Jenkins \u0026amp; ArgoCD Was responsible for setting up an automated Jenkins Pipeline for Lambda Deployment. Dockerized Node Js and Ruby on Rails Applications and pushed them to ECR utilizing efficient image tagging and optimized image size by using Multistage Dockerfile.","title":"DevOps Engineer"},{"content":"Description Server Configuration and Support Writing shell scripts as per requirement for automation Responding to employees\u0026rsquo; IT support requests via taking systems remotely ","permalink":"https://nithin-john-george.github.io/experience/geojit/","summary":"Description Server Configuration and Support Writing shell scripts as per requirement for automation Responding to employees\u0026rsquo; IT support requests via taking systems remotely ","title":"System Engineer"},{"content":" Linux Certification RHCE 7 , RHCSA 7 Cloud AWS , GCP Infrastructure as Code (IAC) Terraform , Terraform Cloud Programming Python Scripting Bash Scripting Version Control System Git , Github CI/CD Jenkins , Github Actions , ArgoCD + ESO Containerization Docker Container Orchestration Kuberenets Kuberenets Templating Helm Charts Service Mesh Istio Kuberenets Autoscaler Karpenter AWS Cloud Services [ IAM , VPC , RDS , EKS , ECS , ECR , Lambda , EventBridge Rules , API Gateway , Step Functions , Elastic Load Balancer (ELB) , Elastic Block Store (EBS) , Elastic File System (EFS) , Amazon S3 , Amazon SNS , Amazon SQS , CloudWatch , Amazon EC2 , AWS SSM, Secrets Manager , CloudFront , Cost Explorer] Code Quality Sonar Qube Secrets Management Tool [Hashicorp Vault, AWS Paramter Store, Bitnami Sealed Secrets] Alerting and Oncall Tool OpsGenie Ticketing Jira Service Management , Clickup Web Security Cloudflare for rate limiting, setting WAF rules, Pointing DNS records ","permalink":"https://nithin-john-george.github.io/technical-skills/technical-skills/","summary":" Linux Certification RHCE 7 , RHCSA 7 Cloud AWS , GCP Infrastructure as Code (IAC) Terraform , Terraform Cloud Programming Python Scripting Bash Scripting Version Control System Git , Github CI/CD Jenkins , Github Actions , ArgoCD + ESO Containerization Docker Container Orchestration Kuberenets Kuberenets Templating Helm Charts Service Mesh Istio Kuberenets Autoscaler Karpenter AWS Cloud Services [ IAM , VPC , RDS , EKS , ECS , ECR , Lambda , EventBridge Rules , API Gateway , Step Functions , Elastic Load Balancer (ELB) , Elastic Block Store (EBS) , Elastic File System (EFS) , Amazon S3 , Amazon SNS , Amazon SQS , CloudWatch , Amazon EC2 , AWS SSM, Secrets Manager , CloudFront , Cost Explorer] Code Quality Sonar Qube Secrets Management Tool [Hashicorp Vault, AWS Paramter Store, Bitnami Sealed Secrets] Alerting and Oncall Tool OpsGenie Ticketing Jira Service Management , Clickup Web Security Cloudflare for rate limiting, setting WAF rules, Pointing DNS records ","title":""}]